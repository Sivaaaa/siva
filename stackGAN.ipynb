{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stackGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1OpXUAslC_OItvlwgOXDKOhUwRXpKg8lG",
      "authorship_tag": "ABX9TyMYNDgENP3MFVgyklDutv8B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sivaaaa/siva/blob/master/stackGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac3vV2PD2V2A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "d733b44c-ad35-4121-b012-21d628d30f73"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "\n",
        "import PIL\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from keras import Input, Model\n",
        "from keras import backend as K\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.layers import Dense, LeakyReLU, BatchNormalization, ReLU, Reshape, UpSampling2D, Conv2D, Activation, \\\n",
        "    concatenate, Flatten, Lambda, Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8rhT1eC3mXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_class_ids(class_info_file_path):\n",
        "    \"\"\"\n",
        "    Load class ids from class_info.pickle file\n",
        "    \"\"\"\n",
        "    with open(class_info_file_path, 'rb') as f:\n",
        "        class_ids = pickle.load(f, encoding='latin1')\n",
        "        return class_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u6RVw0M3uSg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_embeddings(embeddings_file_path):\n",
        "    \"\"\"\n",
        "    Load embeddings\n",
        "    \"\"\"\n",
        "    with open(embeddings_file_path, 'rb') as f:\n",
        "        embeddings = pickle.load(f, encoding='latin1')\n",
        "        embeddings = np.array(embeddings)\n",
        "        print('embeddings: ', embeddings.shape)\n",
        "    return embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqrnO7Nk3w5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_filenames(filenames_file_path):\n",
        "    \"\"\"\n",
        "    Load filenames.pickle file and return a list of all file names\n",
        "    \"\"\"\n",
        "    with open(filenames_file_path, 'rb') as f:\n",
        "        filenames = pickle.load(f, encoding='latin1')\n",
        "    return filenames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSrTMuvf329O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_bounding_boxes(dataset_dir):\n",
        "    \"\"\"\n",
        "    Load bounding boxes and return a dictionary of file names and corresponding bounding boxes\n",
        "    \"\"\"\n",
        "    # Paths\n",
        "    bounding_boxes_path = os.path.join(dataset_dir, 'bounding_boxes.txt')\n",
        "    file_paths_path = os.path.join(dataset_dir, 'images.txt')\n",
        "\n",
        "    # Read bounding_boxes.txt and images.txt file\n",
        "    df_bounding_boxes = pd.read_csv(bounding_boxes_path,\n",
        "                                    delim_whitespace=True, header=None).astype(int)\n",
        "    df_file_names = pd.read_csv(file_paths_path, delim_whitespace=True, header=None)\n",
        "\n",
        "    # Create a list of file names\n",
        "    file_names = df_file_names[1].tolist()\n",
        "\n",
        "    # Create a dictionary of file_names and bounding boxes\n",
        "    filename_boundingbox_dict = {img_file[:-4]: [] for img_file in file_names[:2]}\n",
        "\n",
        "    # Assign a bounding box to the corresponding image\n",
        "    for i in range(0, len(file_names)):\n",
        "        # Get the bounding box\n",
        "        bounding_box = df_bounding_boxes.iloc[i][1:].tolist()\n",
        "        key = file_names[i][:-4]\n",
        "        filename_boundingbox_dict[key] = bounding_box\n",
        "\n",
        "    return filename_boundingbox_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV-W4Mla39Ax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_img(img_path, bbox, image_size):\n",
        "    \"\"\"\n",
        "    Load and resize image\n",
        "    \"\"\"\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    width, height = img.size\n",
        "    if bbox is not None:\n",
        "        R = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
        "        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
        "        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
        "        y1 = np.maximum(0, center_y - R)\n",
        "        y2 = np.minimum(height, center_y + R)\n",
        "        x1 = np.maximum(0, center_x - R)\n",
        "        x2 = np.minimum(width, center_x + R)\n",
        "        img = img.crop([x1, y1, x2, y2])\n",
        "    img = img.resize(image_size, PIL.Image.BILINEAR)\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQqAKtJW3-Oa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(filenames_file_path, class_info_file_path, cub_dataset_dir, embeddings_file_path, image_size):\n",
        "    \"\"\"\n",
        "    Load dataset\n",
        "    \"\"\"\n",
        "    filenames = load_filenames(filenames_file_path)\n",
        "    class_ids = load_class_ids(class_info_file_path)\n",
        "    bounding_boxes = load_bounding_boxes(cub_dataset_dir)\n",
        "    all_embeddings = load_embeddings(embeddings_file_path)\n",
        "\n",
        "    X, y, embeddings = [], [], []\n",
        "\n",
        "    print(\"Embeddings shape:\", all_embeddings.shape)\n",
        "\n",
        "    for index, filename in enumerate(filenames):\n",
        "        bounding_box = bounding_boxes[filename]\n",
        "\n",
        "        try:\n",
        "            # Load images\n",
        "            img_name = '{}/images/{}.jpg'.format(cub_dataset_dir, filename)\n",
        "            img = get_img(img_name, bounding_box, image_size)\n",
        "\n",
        "            all_embeddings1 = all_embeddings[index, :, :]\n",
        "\n",
        "            embedding_ix = random.randint(0, all_embeddings1.shape[0] - 1)\n",
        "            embedding = all_embeddings1[embedding_ix, :]\n",
        "\n",
        "            X.append(np.array(img))\n",
        "            y.append(class_ids[index])\n",
        "            embeddings.append(embedding)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    embeddings = np.array(embeddings)\n",
        "    return X, y, embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXQhnz2O4DoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_c(x):\n",
        "    mean = x[:, :128]\n",
        "    log_sigma = x[:, 128:]\n",
        "    stddev = K.exp(log_sigma)\n",
        "    epsilon = K.random_normal(shape=K.constant((mean.shape[1],), dtype='int32'))\n",
        "    c = stddev * epsilon + mean\n",
        "    return c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvyw0m8s4Iur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_ca_model():\n",
        "    \"\"\"\n",
        "    Get conditioning augmentation model.\n",
        "    Takes an embedding of shape (1024,) and returns a tensor of shape (256,)\n",
        "    \"\"\"\n",
        "    input_layer = Input(shape=(1024,))\n",
        "    x = Dense(256)(input_layer)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    model = Model(inputs=[input_layer], outputs=[x])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD2tpU384d-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_embedding_compressor_model():\n",
        "    \"\"\"\n",
        "    Build embedding compressor model\n",
        "    \"\"\"\n",
        "    input_layer = Input(shape=(1024,))\n",
        "    x = Dense(128)(input_layer)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    model = Model(inputs=[input_layer], outputs=[x])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwH6YKxh4hrw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_stage1_generator():\n",
        "    \"\"\"\n",
        "    Builds a generator model used in Stage-I\n",
        "    \"\"\"\n",
        "    input_layer = Input(shape=(1024,))\n",
        "    x = Dense(256)(input_layer)\n",
        "    mean_logsigma = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    c = Lambda(generate_c)(mean_logsigma)\n",
        "\n",
        "    input_layer2 = Input(shape=(100,))\n",
        "\n",
        "    gen_input = Concatenate(axis=1)([c, input_layer2])\n",
        "\n",
        "    x = Dense(128 * 8 * 4 * 4, use_bias=False)(gen_input)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = Reshape((4, 4, 128 * 8), input_shape=(128 * 8 * 4 * 4,))(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(512, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(256, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(128, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(64, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = Conv2D(3, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = Activation(activation='tanh')(x)\n",
        "\n",
        "    stage1_gen = Model(inputs=[input_layer, input_layer2], outputs=[x, mean_logsigma])\n",
        "    return stage1_gen\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnwPCqrA4kWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_stage1_discriminator():\n",
        "    \"\"\"\n",
        "    Create a model which takes two inputs\n",
        "    1. One from above network\n",
        "    2. One from the embedding layer\n",
        "    3. Concatenate along the axis dimension and feed it to the last module which produces final logits\n",
        "    \"\"\"\n",
        "    input_layer = Input(shape=(64, 64, 3))\n",
        "\n",
        "    x = Conv2D(64, (4, 4),\n",
        "               padding='same', strides=2,\n",
        "               input_shape=(64, 64, 3), use_bias=False)(input_layer)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(128, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(256, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(512, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    input_layer2 = Input(shape=(4, 4, 128))\n",
        "\n",
        "    merged_input = concatenate([x, input_layer2])\n",
        "\n",
        "    x2 = Conv2D(64 * 8, kernel_size=1,\n",
        "                padding=\"same\", strides=1)(merged_input)\n",
        "    x2 = BatchNormalization()(x2)\n",
        "    x2 = LeakyReLU(alpha=0.2)(x2)\n",
        "    x2 = Flatten()(x2)\n",
        "    x2 = Dense(1)(x2)\n",
        "    x2 = Activation('sigmoid')(x2)\n",
        "\n",
        "    stage1_dis = Model(inputs=[input_layer, input_layer2], outputs=[x2])\n",
        "    return stage1_dis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewov7P-N4nqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_adversarial_model(gen_model, dis_model):\n",
        "    input_layer = Input(shape=(1024,))\n",
        "    input_layer2 = Input(shape=(100,))\n",
        "    input_layer3 = Input(shape=(4, 4, 128))\n",
        "\n",
        "    x, mean_logsigma = gen_model([input_layer, input_layer2])\n",
        "\n",
        "    dis_model.trainable = False\n",
        "    valid = dis_model([x, input_layer3])\n",
        "\n",
        "    model = Model(inputs=[input_layer, input_layer2, input_layer3], outputs=[valid, mean_logsigma])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDMjohaO4q0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def KL_loss(y_true, y_pred):\n",
        "    mean = y_pred[:, :128]\n",
        "    logsigma = y_pred[:, :128]\n",
        "    loss = -logsigma + .5 * (-1 + K.exp(2. * logsigma) + K.square(mean))\n",
        "    loss = K.mean(loss)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qdcz3MOb4xIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_rgb_img(img, path):\n",
        "    \"\"\"\n",
        "    Save an rgb image\n",
        "    \"\"\"\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.imshow(img)\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(\"Image\")\n",
        "\n",
        "    plt.savefig(path)\n",
        "    plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GH7s1n-35uo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_log(callback, name, loss, batch_no):\n",
        "    \"\"\"\n",
        "    Write training summary to TensorBoard\n",
        "    \"\"\"\n",
        "    summary = tf.Summary()\n",
        "    summary_value = summary.value.add()\n",
        "    summary_value.simple_value = loss\n",
        "    summary_value.tag = name\n",
        "    callback.writer.add_summary(summary, batch_no)\n",
        "    callback.writer.flush()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oXXP5cq443-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "161be2f5-8242-4f47-981c-03988964f86b"
      },
      "source": [
        "!wget http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-26 08:09:40--  http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz\n",
            "Resolving www.vision.caltech.edu (www.vision.caltech.edu)... 34.208.54.77\n",
            "Connecting to www.vision.caltech.edu (www.vision.caltech.edu)|34.208.54.77|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1150585339 (1.1G) [application/x-tar]\n",
            "Saving to: ‘CUB_200_2011.tgz’\n",
            "\n",
            "CUB_200_2011.tgz    100%[===================>]   1.07G  5.96MB/s    in 2m 37s  \n",
            "\n",
            "2020-03-26 08:12:18 (6.98 MB/s) - ‘CUB_200_2011.tgz’ saved [1150585339/1150585339]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRwx4DeB5K_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tarfile\n",
        "tar = tarfile.open(\"CUB_200_2011.tgz\")\n",
        "tar.extractall()\n",
        "tar.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTBUUYZr5Q6k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e204659-5d6b-4f33-9a24-c68213966242"
      },
      "source": [
        "!ls \"/content/birds\""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "birds.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frlG_Y8b5TJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q \"/content/birds/birds.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63lRHKK75WFa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b10500d9-6f52-4993-d069-704754aa35d8"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "attributes.txt\tCUB_200_2011\t  drive    results2\n",
            "birds\t\tCUB_200_2011.tgz  results  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2eX3Olg5wfp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 28983
        },
        "outputId": "dba21a42-6e9d-4594-d8ac-b34bd92b4de3"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    data_dir = \"/content/birds/\"\n",
        "    train_dir = data_dir + \"/train\"\n",
        "    test_dir = data_dir + \"/test\"\n",
        "    image_size = 64\n",
        "    batch_size = 64\n",
        "    z_dim = 100\n",
        "    stage1_generator_lr = 0.0002\n",
        "    stage1_discriminator_lr = 0.0002\n",
        "    stage1_lr_decay_step = 600\n",
        "    epochs = 2\n",
        "    condition_dim = 128\n",
        "\n",
        "    embeddings_file_path_train = train_dir + \"/char-CNN-RNN-embeddings.pickle\"\n",
        "    embeddings_file_path_test = test_dir + \"/char-CNN-RNN-embeddings.pickle\"\n",
        "\n",
        "    filenames_file_path_train = train_dir + \"/filenames.pickle\"\n",
        "    filenames_file_path_test = test_dir + \"/filenames.pickle\"\n",
        "\n",
        "    class_info_file_path_train = train_dir + \"/class_info.pickle\"\n",
        "    class_info_file_path_test = test_dir + \"/class_info.pickle\"\n",
        "\n",
        "    cub_dataset_dir = \"/content/CUB_200_2011\"\n",
        "    \n",
        "    # Define optimizers\n",
        "    dis_optimizer = Adam(lr=stage1_discriminator_lr, beta_1=0.5, beta_2=0.999)\n",
        "    gen_optimizer = Adam(lr=stage1_generator_lr, beta_1=0.5, beta_2=0.999)\n",
        "\n",
        "    \"\"\"\"\n",
        "    Load datasets\n",
        "    \"\"\"\n",
        "    X_train, y_train, embeddings_train = load_dataset(filenames_file_path=filenames_file_path_train,\n",
        "                                                      class_info_file_path=class_info_file_path_train,\n",
        "                                                      cub_dataset_dir=cub_dataset_dir,\n",
        "                                                      embeddings_file_path=embeddings_file_path_train,\n",
        "                                                      image_size=(64, 64))\n",
        "\n",
        "    X_test, y_test, embeddings_test = load_dataset(filenames_file_path=filenames_file_path_test,\n",
        "                                                   class_info_file_path=class_info_file_path_test,\n",
        "                                                   cub_dataset_dir=cub_dataset_dir,\n",
        "                                                   embeddings_file_path=embeddings_file_path_test,\n",
        "                                                   image_size=(64, 64))\n",
        "\n",
        "    \"\"\"\n",
        "    Build and compile networks\n",
        "    \"\"\"\n",
        "    ca_model = build_ca_model()\n",
        "    ca_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
        "\n",
        "    stage1_dis = build_stage1_discriminator()\n",
        "    stage1_dis.compile(loss='binary_crossentropy', optimizer=dis_optimizer)\n",
        "\n",
        "    stage1_gen = build_stage1_generator()\n",
        "    stage1_gen.compile(loss=\"mse\", optimizer=gen_optimizer)\n",
        "\n",
        "    embedding_compressor_model = build_embedding_compressor_model()\n",
        "    embedding_compressor_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
        "\n",
        "    adversarial_model = build_adversarial_model(gen_model=stage1_gen, dis_model=stage1_dis)\n",
        "    adversarial_model.compile(loss=['binary_crossentropy', KL_loss], loss_weights=[1, 2.0],\n",
        "                              optimizer=gen_optimizer, metrics=None)\n",
        "\n",
        "    tensorboard = TensorBoard(log_dir=\"logs/\".format(time.time()))\n",
        "    tensorboard.set_model(stage1_gen)\n",
        "    tensorboard.set_model(stage1_dis)\n",
        "    tensorboard.set_model(ca_model)\n",
        "    tensorboard.set_model(embedding_compressor_model)\n",
        "\n",
        "    # Generate an array containing real and fake values\n",
        "    # Apply label smoothing as well\n",
        "    real_labels = np.ones((batch_size, 1), dtype=float) * 0.9\n",
        "    fake_labels = np.zeros((batch_size, 1), dtype=float) * 0.1\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(\"========================================\")\n",
        "        print(\"Epoch is:\", epoch)\n",
        "        print(\"Number of batches\", int(X_train.shape[0] / batch_size))\n",
        "\n",
        "        gen_losses = []\n",
        "        dis_losses = []\n",
        "\n",
        "        # Load data and train model\n",
        "        number_of_batches = int(X_train.shape[0] / batch_size)\n",
        "        for index in range(number_of_batches):\n",
        "            print(\"Batch:{}\".format(index+1))\n",
        "            \n",
        "            \"\"\"\n",
        "            Train the discriminator network\n",
        "            \"\"\"\n",
        "            # Sample a batch of data\n",
        "            z_noise = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
        "            image_batch = X_train[index * batch_size:(index + 1) * batch_size]\n",
        "            embedding_batch = embeddings_train[index * batch_size:(index + 1) * batch_size]\n",
        "            image_batch = (image_batch - 127.5) / 127.5\n",
        "\n",
        "            # Generate fake images\n",
        "            fake_images, _ = stage1_gen.predict([embedding_batch, z_noise], verbose=3)\n",
        "\n",
        "            # Generate compressed embeddings\n",
        "            compressed_embedding = embedding_compressor_model.predict_on_batch(embedding_batch)\n",
        "            compressed_embedding = np.reshape(compressed_embedding, (-1, 1, 1, condition_dim))\n",
        "            compressed_embedding = np.tile(compressed_embedding, (1, 4, 4, 1))\n",
        "\n",
        "            dis_loss_real = stage1_dis.train_on_batch([image_batch, compressed_embedding],\n",
        "                                                      np.reshape(real_labels, (batch_size, 1)))\n",
        "            dis_loss_fake = stage1_dis.train_on_batch([fake_images, compressed_embedding],\n",
        "                                                      np.reshape(fake_labels, (batch_size, 1)))\n",
        "            dis_loss_wrong = stage1_dis.train_on_batch([image_batch[:(batch_size - 1)], compressed_embedding[1:]],\n",
        "                                                       np.reshape(fake_labels[1:], (batch_size-1, 1)))\n",
        "\n",
        "            d_loss = 0.5 * np.add(dis_loss_real, 0.5 * np.add(dis_loss_wrong, dis_loss_fake))\n",
        "\n",
        "            print(\"d_loss_real:{}\".format(dis_loss_real))\n",
        "            print(\"d_loss_fake:{}\".format(dis_loss_fake))\n",
        "            print(\"d_loss_wrong:{}\".format(dis_loss_wrong))\n",
        "            print(\"d_loss:{}\".format(d_loss))\n",
        "\n",
        "            \"\"\"\n",
        "            Train the generator network \n",
        "            \"\"\"\n",
        "            g_loss = adversarial_model.train_on_batch([embedding_batch, z_noise, compressed_embedding],[K.ones((batch_size, 1)) * 0.9, K.ones((batch_size, 256)) * 0.9])\n",
        "            print(\"g_loss:{}\".format(g_loss))\n",
        "\n",
        "            dis_losses.append(d_loss)\n",
        "            gen_losses.append(g_loss)\n",
        "\n",
        "        \"\"\"\n",
        "        Save losses to Tensorboard after each epoch\n",
        "        \"\"\"\n",
        "        write_log(tensorboard, 'discriminator_loss', np.mean(dis_losses), epoch)\n",
        "        write_log(tensorboard, 'generator_loss', np.mean(gen_losses[0]), epoch)\n",
        "        \n",
        "        # Generate and save images after every 2nd epoch\n",
        "        if epoch % 2 == 0:\n",
        "            # z_noise2 = np.random.uniform(-1, 1, size=(batch_size, z_dim))\n",
        "            z_noise2 = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
        "            embedding_batch = embeddings_test[0:batch_size]\n",
        "            fake_images, _ = stage1_gen.predict_on_batch([embedding_batch, z_noise2])\n",
        "\n",
        "            # Save images\n",
        "            for i, img in enumerate(fake_images[:10]):\n",
        "                save_rgb_img(img, \"results/gen_{}_{}.png\".format(epoch, i))\n",
        "\n",
        "    # Save models\n",
        "    stage1_gen.save_weights(\"stage1_gen.h5\")\n",
        "    stage1_dis.save_weights(\"stage1_dis.h5\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embeddings:  (8855, 10, 1024)\n",
            "Embeddings shape: (8855, 10, 1024)\n",
            "embeddings:  (2933, 10, 1024)\n",
            "Embeddings shape: (2933, 10, 1024)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4409: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "========================================\n",
            "Epoch is: 0\n",
            "Number of batches 138\n",
            "Batch:1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "d_loss_real:0.5572655200958252\n",
            "d_loss_fake:4.291682243347168\n",
            "d_loss_wrong:5.749064922332764\n",
            "d_loss:2.788819432258606\n",
            "g_loss:[2.123328, 2.08502, 0.019153915]\n",
            "Batch:2\n",
            "d_loss_real:0.9786112308502197\n",
            "d_loss_fake:1.3467211723327637\n",
            "d_loss_wrong:0.5569629073143005\n",
            "d_loss:0.9652266502380371\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "g_loss:[2.771244, 2.7295444, 0.020849794]\n",
            "Batch:3\n",
            "d_loss_real:3.6091134548187256\n",
            "d_loss_fake:0.016226574778556824\n",
            "d_loss_wrong:1.345731258392334\n",
            "d_loss:2.1450461745262146\n",
            "g_loss:[1.8805792, 1.831687, 0.024446102]\n",
            "Batch:4\n",
            "d_loss_real:2.0064823627471924\n",
            "d_loss_fake:0.028077788650989532\n",
            "d_loss_wrong:3.04638934135437\n",
            "d_loss:1.7718579769134521\n",
            "g_loss:[1.555896, 1.5094206, 0.023237735]\n",
            "Batch:5\n",
            "d_loss_real:1.48604154586792\n",
            "d_loss_fake:0.04514239728450775\n",
            "d_loss_wrong:2.182438611984253\n",
            "d_loss:1.2999160289764404\n",
            "g_loss:[2.365866, 2.3280656, 0.018900156]\n",
            "Batch:6\n",
            "d_loss_real:2.3461861610412598\n",
            "d_loss_fake:0.10150869190692902\n",
            "d_loss_wrong:1.8508524894714355\n",
            "d_loss:1.661183387041092\n",
            "g_loss:[2.7271123, 2.6861908, 0.020460723]\n",
            "Batch:7\n",
            "d_loss_real:2.3411409854888916\n",
            "d_loss_fake:0.0014876732602715492\n",
            "d_loss_wrong:1.2574573755264282\n",
            "d_loss:1.4853067696094513\n",
            "g_loss:[2.496869, 2.4619765, 0.017446239]\n",
            "Batch:8\n",
            "d_loss_real:2.5118257999420166\n",
            "d_loss_fake:0.05518210679292679\n",
            "d_loss_wrong:0.646571159362793\n",
            "d_loss:1.431351214647293\n",
            "g_loss:[1.5598266, 1.5041559, 0.027835343]\n",
            "Batch:9\n",
            "d_loss_real:1.8529250621795654\n",
            "d_loss_fake:0.5436734557151794\n",
            "d_loss_wrong:1.0976026058197021\n",
            "d_loss:1.3367815613746643\n",
            "g_loss:[6.438879, 6.3558807, 0.041499186]\n",
            "Batch:10\n",
            "d_loss_real:1.8678877353668213\n",
            "d_loss_fake:0.008365929126739502\n",
            "d_loss_wrong:1.0042223930358887\n",
            "d_loss:1.1870909333229065\n",
            "g_loss:[5.8506083, 5.7874594, 0.031574525]\n",
            "Batch:11\n",
            "d_loss_real:1.6149659156799316\n",
            "d_loss_fake:0.014329256489872932\n",
            "d_loss_wrong:1.1012279987335205\n",
            "d_loss:1.086372286081314\n",
            "g_loss:[2.524217, 2.4789, 0.0226585]\n",
            "Batch:12\n",
            "d_loss_real:1.5878570079803467\n",
            "d_loss_fake:0.7143216729164124\n",
            "d_loss_wrong:0.7069204449653625\n",
            "d_loss:1.149239033460617\n",
            "g_loss:[6.4854665, 6.4416103, 0.02192798]\n",
            "Batch:13\n",
            "d_loss_real:1.8631938695907593\n",
            "d_loss_fake:0.03389880061149597\n",
            "d_loss_wrong:0.7730502486228943\n",
            "d_loss:1.1333341896533966\n",
            "g_loss:[5.3349915, 5.295484, 0.01975372]\n",
            "Batch:14\n",
            "d_loss_real:1.5291118621826172\n",
            "d_loss_fake:0.10966458916664124\n",
            "d_loss_wrong:0.9754591584205627\n",
            "d_loss:1.0358368754386902\n",
            "g_loss:[3.154747, 3.1254482, 0.014649375]\n",
            "Batch:15\n",
            "d_loss_real:1.496696949005127\n",
            "d_loss_fake:0.16073152422904968\n",
            "d_loss_wrong:1.1704835891723633\n",
            "d_loss:1.0811522603034973\n",
            "g_loss:[3.168227, 3.108561, 0.029832963]\n",
            "Batch:16\n",
            "d_loss_real:1.2655258178710938\n",
            "d_loss_fake:0.36483079195022583\n",
            "d_loss_wrong:0.7575687766075134\n",
            "d_loss:0.9133628010749817\n",
            "g_loss:[4.222597, 4.1747894, 0.023903904]\n",
            "Batch:17\n",
            "d_loss_real:1.7012627124786377\n",
            "d_loss_fake:0.015938010066747665\n",
            "d_loss_wrong:1.5187833309173584\n",
            "d_loss:1.2343116998672485\n",
            "g_loss:[4.0541005, 4.009145, 0.02247782]\n",
            "Batch:18\n",
            "d_loss_real:1.5818849802017212\n",
            "d_loss_fake:0.017398208379745483\n",
            "d_loss_wrong:0.7017716765403748\n",
            "d_loss:0.9707349538803101\n",
            "g_loss:[1.9525589, 1.9199902, 0.016284358]\n",
            "Batch:19\n",
            "d_loss_real:1.5333458185195923\n",
            "d_loss_fake:0.17795902490615845\n",
            "d_loss_wrong:0.7416378259658813\n",
            "d_loss:0.9965721219778061\n",
            "g_loss:[3.8686383, 3.8368075, 0.015915414]\n",
            "Batch:20\n",
            "d_loss_real:1.6500627994537354\n",
            "d_loss_fake:0.08572334051132202\n",
            "d_loss_wrong:0.8415120244026184\n",
            "d_loss:1.0568402409553528\n",
            "g_loss:[1.304615, 1.2664503, 0.01908239]\n",
            "Batch:21\n",
            "d_loss_real:1.5158571004867554\n",
            "d_loss_fake:0.18401673436164856\n",
            "d_loss_wrong:0.6006649136543274\n",
            "d_loss:0.9540989696979523\n",
            "g_loss:[2.5730112, 2.5390902, 0.016960476]\n",
            "Batch:22\n",
            "d_loss_real:1.2927751541137695\n",
            "d_loss_fake:0.08229050785303116\n",
            "d_loss_wrong:0.9234712719917297\n",
            "d_loss:0.8978280127048492\n",
            "g_loss:[3.4369013, 3.4081478, 0.014376743]\n",
            "Batch:23\n",
            "d_loss_real:1.2935359477996826\n",
            "d_loss_fake:0.06733563542366028\n",
            "d_loss_wrong:0.7476298213005066\n",
            "d_loss:0.8505093455314636\n",
            "g_loss:[2.501549, 2.4674077, 0.01707064]\n",
            "Batch:24\n",
            "d_loss_real:1.0548095703125\n",
            "d_loss_fake:0.014135087840259075\n",
            "d_loss_wrong:0.938791036605835\n",
            "d_loss:0.7656363099813461\n",
            "g_loss:[2.0074933, 1.9529756, 0.027258828]\n",
            "Batch:25\n",
            "d_loss_real:1.2523974180221558\n",
            "d_loss_fake:0.013052834197878838\n",
            "d_loss_wrong:0.9315674304962158\n",
            "d_loss:0.8623537719249725\n",
            "g_loss:[2.3616009, 2.3179607, 0.02182008]\n",
            "Batch:26\n",
            "d_loss_real:1.2027454376220703\n",
            "d_loss_fake:0.02088334411382675\n",
            "d_loss_wrong:0.8733088374137878\n",
            "d_loss:0.8249207586050034\n",
            "g_loss:[0.9435581, 0.909091, 0.017233562]\n",
            "Batch:27\n",
            "d_loss_real:1.2588379383087158\n",
            "d_loss_fake:0.013855822384357452\n",
            "d_loss_wrong:1.0740971565246582\n",
            "d_loss:0.9014072120189667\n",
            "g_loss:[2.0296881, 1.9900787, 0.01980467]\n",
            "Batch:28\n",
            "d_loss_real:0.9956585764884949\n",
            "d_loss_fake:0.045295558869838715\n",
            "d_loss_wrong:0.8802161812782288\n",
            "d_loss:0.7292072176933289\n",
            "g_loss:[1.3209215, 1.2628471, 0.029037237]\n",
            "Batch:29\n",
            "d_loss_real:1.0892269611358643\n",
            "d_loss_fake:0.0358838215470314\n",
            "d_loss_wrong:0.7704092860221863\n",
            "d_loss:0.746186763048172\n",
            "g_loss:[1.680201, 1.6214345, 0.02938329]\n",
            "Batch:30\n",
            "d_loss_real:1.133088231086731\n",
            "d_loss_fake:0.014431076124310493\n",
            "d_loss_wrong:1.0757700204849243\n",
            "d_loss:0.8390944004058838\n",
            "g_loss:[1.6694145, 1.6237283, 0.022843095]\n",
            "Batch:31\n",
            "d_loss_real:0.9883788824081421\n",
            "d_loss_fake:0.10528405010700226\n",
            "d_loss_wrong:0.7587838768959045\n",
            "d_loss:0.7102064192295074\n",
            "g_loss:[1.3841211, 1.3445262, 0.019797426]\n",
            "Batch:32\n",
            "d_loss_real:1.1861897706985474\n",
            "d_loss_fake:0.0676422193646431\n",
            "d_loss_wrong:0.6574653387069702\n",
            "d_loss:0.7743717730045319\n",
            "g_loss:[1.7684549, 1.7130129, 0.027721014]\n",
            "Batch:33\n",
            "d_loss_real:1.2266223430633545\n",
            "d_loss_fake:0.08440136909484863\n",
            "d_loss_wrong:0.6896488070487976\n",
            "d_loss:0.8068237155675888\n",
            "g_loss:[1.4637028, 1.4124742, 0.025614293]\n",
            "Batch:34\n",
            "d_loss_real:1.2272396087646484\n",
            "d_loss_fake:0.12102842330932617\n",
            "d_loss_wrong:0.6545928716659546\n",
            "d_loss:0.8075251281261444\n",
            "g_loss:[2.2374194, 2.1850502, 0.026184559]\n",
            "Batch:35\n",
            "d_loss_real:1.195317029953003\n",
            "d_loss_fake:0.12037329375743866\n",
            "d_loss_wrong:0.7160009741783142\n",
            "d_loss:0.80675208568573\n",
            "g_loss:[3.152698, 3.1108594, 0.020919308]\n",
            "Batch:36\n",
            "d_loss_real:1.231701135635376\n",
            "d_loss_fake:0.03848396986722946\n",
            "d_loss_wrong:0.7083960771560669\n",
            "d_loss:0.8025705814361572\n",
            "g_loss:[2.4003134, 2.3501363, 0.025088554]\n",
            "Batch:37\n",
            "d_loss_real:1.1931194067001343\n",
            "d_loss_fake:0.06529241800308228\n",
            "d_loss_wrong:0.8347739577293396\n",
            "d_loss:0.8215762972831726\n",
            "g_loss:[1.7481678, 1.7044458, 0.021860927]\n",
            "Batch:38\n",
            "d_loss_real:1.1432538032531738\n",
            "d_loss_fake:0.0497654527425766\n",
            "d_loss_wrong:0.8352484703063965\n",
            "d_loss:0.7928803861141205\n",
            "g_loss:[2.0881605, 2.041803, 0.023178868]\n",
            "Batch:39\n",
            "d_loss_real:1.0488895177841187\n",
            "d_loss_fake:0.0845225378870964\n",
            "d_loss_wrong:0.7545080184936523\n",
            "d_loss:0.7342023998498917\n",
            "g_loss:[2.1693683, 2.125482, 0.02194307]\n",
            "Batch:40\n",
            "d_loss_real:1.1364004611968994\n",
            "d_loss_fake:0.044152840971946716\n",
            "d_loss_wrong:1.0128062963485718\n",
            "d_loss:0.8324400186538696\n",
            "g_loss:[2.2064795, 2.1753669, 0.015556328]\n",
            "Batch:41\n",
            "d_loss_real:1.0639454126358032\n",
            "d_loss_fake:0.0672176405787468\n",
            "d_loss_wrong:0.6970340013504028\n",
            "d_loss:0.7230356186628342\n",
            "g_loss:[1.8807483, 1.8438644, 0.018441942]\n",
            "Batch:42\n",
            "d_loss_real:1.3571518659591675\n",
            "d_loss_fake:0.11002947390079498\n",
            "d_loss_wrong:0.5948387980461121\n",
            "d_loss:0.8547929972410202\n",
            "g_loss:[1.8734461, 1.8359399, 0.018753111]\n",
            "Batch:43\n",
            "d_loss_real:1.0353126525878906\n",
            "d_loss_fake:0.10439354181289673\n",
            "d_loss_wrong:0.6885833740234375\n",
            "d_loss:0.7159005552530289\n",
            "g_loss:[2.2938104, 2.2469711, 0.023419643]\n",
            "Batch:44\n",
            "d_loss_real:0.9693834781646729\n",
            "d_loss_fake:0.12195567786693573\n",
            "d_loss_wrong:0.7312436103820801\n",
            "d_loss:0.6979915648698807\n",
            "g_loss:[2.024542, 1.985698, 0.019422097]\n",
            "Batch:45\n",
            "d_loss_real:1.1143748760223389\n",
            "d_loss_fake:0.04630446806550026\n",
            "d_loss_wrong:0.6455444097518921\n",
            "d_loss:0.730149656534195\n",
            "g_loss:[2.5122747, 2.4661145, 0.023080088]\n",
            "Batch:46\n",
            "d_loss_real:1.0997314453125\n",
            "d_loss_fake:0.05106716230511665\n",
            "d_loss_wrong:0.7037704586982727\n",
            "d_loss:0.7385751307010651\n",
            "g_loss:[2.106208, 2.0637636, 0.02122228]\n",
            "Batch:47\n",
            "d_loss_real:1.1463072299957275\n",
            "d_loss_fake:0.09760572016239166\n",
            "d_loss_wrong:0.7198356986045837\n",
            "d_loss:0.7775139659643173\n",
            "g_loss:[1.9549056, 1.9170339, 0.018935882]\n",
            "Batch:48\n",
            "d_loss_real:1.1404750347137451\n",
            "d_loss_fake:0.10774879902601242\n",
            "d_loss_wrong:0.6317453980445862\n",
            "d_loss:0.7551110684871674\n",
            "g_loss:[2.0359771, 1.989681, 0.02314806]\n",
            "Batch:49\n",
            "d_loss_real:1.1226015090942383\n",
            "d_loss_fake:0.04014497995376587\n",
            "d_loss_wrong:0.6611210107803345\n",
            "d_loss:0.7366172522306442\n",
            "g_loss:[2.3008943, 2.251739, 0.024577584]\n",
            "Batch:50\n",
            "d_loss_real:1.008094310760498\n",
            "d_loss_fake:0.04018153250217438\n",
            "d_loss_wrong:0.6911459565162659\n",
            "d_loss:0.6868790239095688\n",
            "g_loss:[1.6661896, 1.6316175, 0.017285993]\n",
            "Batch:51\n",
            "d_loss_real:1.0819175243377686\n",
            "d_loss_fake:0.04621686041355133\n",
            "d_loss_wrong:0.7611634731292725\n",
            "d_loss:0.7428038418292999\n",
            "g_loss:[1.5191611, 1.4884231, 0.015369019]\n",
            "Batch:52\n",
            "d_loss_real:1.0265858173370361\n",
            "d_loss_fake:0.04948187246918678\n",
            "d_loss_wrong:0.7613940238952637\n",
            "d_loss:0.7160118818283081\n",
            "g_loss:[1.7989109, 1.7730242, 0.012943326]\n",
            "Batch:53\n",
            "d_loss_real:1.1087887287139893\n",
            "d_loss_fake:0.037994660437107086\n",
            "d_loss_wrong:0.7003346085548401\n",
            "d_loss:0.7389766871929169\n",
            "g_loss:[1.5002319, 1.4622817, 0.01897508]\n",
            "Batch:54\n",
            "d_loss_real:0.8902001976966858\n",
            "d_loss_fake:0.0661543607711792\n",
            "d_loss_wrong:0.771310567855835\n",
            "d_loss:0.6544663310050964\n",
            "g_loss:[2.5887308, 2.5585341, 0.015098365]\n",
            "Batch:55\n",
            "d_loss_real:1.0075345039367676\n",
            "d_loss_fake:0.04987422376871109\n",
            "d_loss_wrong:0.6848352551460266\n",
            "d_loss:0.6874446272850037\n",
            "g_loss:[2.0764751, 2.0389352, 0.018769938]\n",
            "Batch:56\n",
            "d_loss_real:1.1557221412658691\n",
            "d_loss_fake:0.04215838015079498\n",
            "d_loss_wrong:0.7259915471076965\n",
            "d_loss:0.7698985487222672\n",
            "g_loss:[1.560905, 1.5180302, 0.021437436]\n",
            "Batch:57\n",
            "d_loss_real:1.1310456991195679\n",
            "d_loss_fake:0.047607820481061935\n",
            "d_loss_wrong:0.6895952224731445\n",
            "d_loss:0.7498236149549484\n",
            "g_loss:[1.5837845, 1.5331073, 0.025338579]\n",
            "Batch:58\n",
            "d_loss_real:0.9362798929214478\n",
            "d_loss_fake:0.048963550478219986\n",
            "d_loss_wrong:0.7648795247077942\n",
            "d_loss:0.6716007143259048\n",
            "g_loss:[1.7392694, 1.7014421, 0.018913649]\n",
            "Batch:59\n",
            "d_loss_real:1.0546072721481323\n",
            "d_loss_fake:0.05471324548125267\n",
            "d_loss_wrong:0.753059983253479\n",
            "d_loss:0.7292469441890717\n",
            "g_loss:[1.9587581, 1.9135163, 0.022620916]\n",
            "Batch:60\n",
            "d_loss_real:0.9893779158592224\n",
            "d_loss_fake:0.04085419327020645\n",
            "d_loss_wrong:0.7549179196357727\n",
            "d_loss:0.6936319917440414\n",
            "g_loss:[1.9577078, 1.9130025, 0.02235263]\n",
            "Batch:61\n",
            "d_loss_real:1.072538137435913\n",
            "d_loss_fake:0.023776620626449585\n",
            "d_loss_wrong:0.7347309589385986\n",
            "d_loss:0.7258959710597992\n",
            "g_loss:[1.3412421, 1.311202, 0.015020003]\n",
            "Batch:62\n",
            "d_loss_real:1.0350115299224854\n",
            "d_loss_fake:0.05162406340241432\n",
            "d_loss_wrong:0.7242969274520874\n",
            "d_loss:0.7114860117435455\n",
            "g_loss:[1.5872712, 1.5539129, 0.016679185]\n",
            "Batch:63\n",
            "d_loss_real:1.1505162715911865\n",
            "d_loss_fake:0.054185833781957626\n",
            "d_loss_wrong:0.6772220730781555\n",
            "d_loss:0.7581101059913635\n",
            "g_loss:[1.5454955, 1.5048468, 0.02032438]\n",
            "Batch:64\n",
            "d_loss_real:1.0302419662475586\n",
            "d_loss_fake:0.09280169010162354\n",
            "d_loss_wrong:0.689676821231842\n",
            "d_loss:0.7107406109571457\n",
            "g_loss:[1.4636489, 1.4290245, 0.0173122]\n",
            "Batch:65\n",
            "d_loss_real:1.0528266429901123\n",
            "d_loss_fake:0.07297094166278839\n",
            "d_loss_wrong:0.7264828085899353\n",
            "d_loss:0.7262767553329468\n",
            "g_loss:[1.5594627, 1.5167323, 0.021365188]\n",
            "Batch:66\n",
            "d_loss_real:0.9652315378189087\n",
            "d_loss_fake:0.1594456434249878\n",
            "d_loss_wrong:0.6573778390884399\n",
            "d_loss:0.6868216395378113\n",
            "g_loss:[1.8233422, 1.7948889, 0.014226692]\n",
            "Batch:67\n",
            "d_loss_real:1.1865272521972656\n",
            "d_loss_fake:0.09420476108789444\n",
            "d_loss_wrong:0.6300424337387085\n",
            "d_loss:0.774325430393219\n",
            "g_loss:[1.7270356, 1.6979764, 0.014529661]\n",
            "Batch:68\n",
            "d_loss_real:1.152901530265808\n",
            "d_loss_fake:0.15991371870040894\n",
            "d_loss_wrong:0.6550009846687317\n",
            "d_loss:0.7801794409751892\n",
            "g_loss:[1.9971719, 1.9670157, 0.015078084]\n",
            "Batch:69\n",
            "d_loss_real:1.2732830047607422\n",
            "d_loss_fake:0.09003829956054688\n",
            "d_loss_wrong:0.9494599103927612\n",
            "d_loss:0.8965160548686981\n",
            "g_loss:[1.7585845, 1.7267416, 0.015921488]\n",
            "Batch:70\n",
            "d_loss_real:0.9866327047348022\n",
            "d_loss_fake:0.06526252627372742\n",
            "d_loss_wrong:0.721916675567627\n",
            "d_loss:0.6901111602783203\n",
            "g_loss:[1.6371037, 1.6044986, 0.016302515]\n",
            "Batch:71\n",
            "d_loss_real:1.103534460067749\n",
            "d_loss_fake:0.16841085255146027\n",
            "d_loss_wrong:0.5567233562469482\n",
            "d_loss:0.7330507785081863\n",
            "g_loss:[1.7664659, 1.7237215, 0.021372186]\n",
            "Batch:72\n",
            "d_loss_real:1.1845827102661133\n",
            "d_loss_fake:0.08163914084434509\n",
            "d_loss_wrong:0.611798107624054\n",
            "d_loss:0.7656506597995758\n",
            "g_loss:[1.9584513, 1.9330112, 0.012720024]\n",
            "Batch:73\n",
            "d_loss_real:0.9971872568130493\n",
            "d_loss_fake:0.13370493054389954\n",
            "d_loss_wrong:0.6631568074226379\n",
            "d_loss:0.6978090703487396\n",
            "g_loss:[1.7903665, 1.7623541, 0.014006201]\n",
            "Batch:74\n",
            "d_loss_real:1.1829092502593994\n",
            "d_loss_fake:0.10159637033939362\n",
            "d_loss_wrong:0.6642004251480103\n",
            "d_loss:0.7829038202762604\n",
            "g_loss:[2.0295513, 2.00376, 0.0128956195]\n",
            "Batch:75\n",
            "d_loss_real:1.0847837924957275\n",
            "d_loss_fake:0.1153736412525177\n",
            "d_loss_wrong:0.6763034462928772\n",
            "d_loss:0.7403111755847931\n",
            "g_loss:[1.7988017, 1.7671784, 0.015811639]\n",
            "Batch:76\n",
            "d_loss_real:1.1353825330734253\n",
            "d_loss_fake:0.1455264687538147\n",
            "d_loss_wrong:0.5950676202774048\n",
            "d_loss:0.7528397887945175\n",
            "g_loss:[2.2192369, 2.1909885, 0.01412414]\n",
            "Batch:77\n",
            "d_loss_real:1.131784439086914\n",
            "d_loss_fake:0.09645518660545349\n",
            "d_loss_wrong:0.6515036225318909\n",
            "d_loss:0.7528819143772125\n",
            "g_loss:[2.0584424, 2.0314102, 0.0135160815]\n",
            "Batch:78\n",
            "d_loss_real:1.0943443775177002\n",
            "d_loss_fake:0.0724845677614212\n",
            "d_loss_wrong:0.630506157875061\n",
            "d_loss:0.7229198664426804\n",
            "g_loss:[2.2696884, 2.2447336, 0.012477423]\n",
            "Batch:79\n",
            "d_loss_real:1.0910228490829468\n",
            "d_loss_fake:0.10515721142292023\n",
            "d_loss_wrong:0.6349744200706482\n",
            "d_loss:0.7305443286895752\n",
            "g_loss:[2.5108588, 2.485444, 0.01270732]\n",
            "Batch:80\n",
            "d_loss_real:0.9589329957962036\n",
            "d_loss_fake:0.11890047043561935\n",
            "d_loss_wrong:0.6768521666526794\n",
            "d_loss:0.6784046590328217\n",
            "g_loss:[2.7552123, 2.7304738, 0.012369232]\n",
            "Batch:81\n",
            "d_loss_real:1.1765615940093994\n",
            "d_loss_fake:0.11958841979503632\n",
            "d_loss_wrong:0.5681275129318237\n",
            "d_loss:0.760209783911705\n",
            "g_loss:[2.5785706, 2.5503948, 0.014087958]\n",
            "Batch:82\n",
            "d_loss_real:1.3750402927398682\n",
            "d_loss_fake:0.20601476728916168\n",
            "d_loss_wrong:0.5947665572166443\n",
            "d_loss:0.8877154737710953\n",
            "g_loss:[2.5112622, 2.4859998, 0.01263116]\n",
            "Batch:83\n",
            "d_loss_real:0.974777102470398\n",
            "d_loss_fake:0.05783528462052345\n",
            "d_loss_wrong:0.6854800581932068\n",
            "d_loss:0.673217386007309\n",
            "g_loss:[2.4079099, 2.3807313, 0.013589294]\n",
            "Batch:84\n",
            "d_loss_real:0.9750816226005554\n",
            "d_loss_fake:0.10508798062801361\n",
            "d_loss_wrong:0.7063330411911011\n",
            "d_loss:0.6903960704803467\n",
            "g_loss:[2.007706, 1.9791905, 0.014257682]\n",
            "Batch:85\n",
            "d_loss_real:1.08612060546875\n",
            "d_loss_fake:0.12916867434978485\n",
            "d_loss_wrong:0.6204312443733215\n",
            "d_loss:0.7304602861404419\n",
            "g_loss:[2.1071591, 2.0846205, 0.01126932]\n",
            "Batch:86\n",
            "d_loss_real:1.0270094871520996\n",
            "d_loss_fake:0.09838157892227173\n",
            "d_loss_wrong:0.6377167701721191\n",
            "d_loss:0.6975293308496475\n",
            "g_loss:[1.8557671, 1.8327429, 0.0115121165]\n",
            "Batch:87\n",
            "d_loss_real:1.0922904014587402\n",
            "d_loss_fake:0.2497284710407257\n",
            "d_loss_wrong:0.5611364245414734\n",
            "d_loss:0.7488614320755005\n",
            "g_loss:[1.646248, 1.6152675, 0.015490217]\n",
            "Batch:88\n",
            "d_loss_real:1.3786245584487915\n",
            "d_loss_fake:0.3081645667552948\n",
            "d_loss_wrong:0.5065441727638245\n",
            "d_loss:0.892989456653595\n",
            "g_loss:[1.6329182, 1.6014402, 0.015739005]\n",
            "Batch:89\n",
            "d_loss_real:1.3004367351531982\n",
            "d_loss_fake:0.21346530318260193\n",
            "d_loss_wrong:0.7474626302719116\n",
            "d_loss:0.8904503583908081\n",
            "g_loss:[2.0671191, 2.0234964, 0.021811314]\n",
            "Batch:90\n",
            "d_loss_real:1.2289307117462158\n",
            "d_loss_fake:0.11945830285549164\n",
            "d_loss_wrong:0.6313883662223816\n",
            "d_loss:0.8021770268678665\n",
            "g_loss:[2.228287, 2.1936283, 0.017329335]\n",
            "Batch:91\n",
            "d_loss_real:1.229102373123169\n",
            "d_loss_fake:0.1731422245502472\n",
            "d_loss_wrong:0.6621261239051819\n",
            "d_loss:0.8233682811260223\n",
            "g_loss:[2.457925, 2.429388, 0.014268475]\n",
            "Batch:92\n",
            "d_loss_real:1.4129900932312012\n",
            "d_loss_fake:0.1837713122367859\n",
            "d_loss_wrong:0.9348310232162476\n",
            "d_loss:0.9861456155776978\n",
            "g_loss:[2.2485085, 2.1965685, 0.025969984]\n",
            "Batch:93\n",
            "d_loss_real:1.108518362045288\n",
            "d_loss_fake:0.1187816858291626\n",
            "d_loss_wrong:0.6575020551681519\n",
            "d_loss:0.7483301162719727\n",
            "g_loss:[2.1469488, 2.1007419, 0.023103468]\n",
            "Batch:94\n",
            "d_loss_real:1.2922101020812988\n",
            "d_loss_fake:0.1731892228126526\n",
            "d_loss_wrong:0.5927323698997498\n",
            "d_loss:0.83758544921875\n",
            "g_loss:[1.5036047, 1.4638584, 0.019873116]\n",
            "Batch:95\n",
            "d_loss_real:1.1798434257507324\n",
            "d_loss_fake:0.17432773113250732\n",
            "d_loss_wrong:0.5906152725219727\n",
            "d_loss:0.7811574637889862\n",
            "g_loss:[1.8908864, 1.8549036, 0.01799145]\n",
            "Batch:96\n",
            "d_loss_real:0.9803282022476196\n",
            "d_loss_fake:0.14760127663612366\n",
            "d_loss_wrong:0.6272119283676147\n",
            "d_loss:0.6838673949241638\n",
            "g_loss:[1.7876695, 1.7493274, 0.019171078]\n",
            "Batch:97\n",
            "d_loss_real:1.157625675201416\n",
            "d_loss_fake:0.08142820000648499\n",
            "d_loss_wrong:0.5969202518463135\n",
            "d_loss:0.748399943113327\n",
            "g_loss:[1.7862188, 1.7476687, 0.019274995]\n",
            "Batch:98\n",
            "d_loss_real:1.1072040796279907\n",
            "d_loss_fake:0.23041385412216187\n",
            "d_loss_wrong:0.6678085327148438\n",
            "d_loss:0.7781576365232468\n",
            "g_loss:[1.7897795, 1.7547071, 0.017536202]\n",
            "Batch:99\n",
            "d_loss_real:1.273634672164917\n",
            "d_loss_fake:0.22330380976200104\n",
            "d_loss_wrong:0.5319240689277649\n",
            "d_loss:0.8256243020296097\n",
            "g_loss:[2.1061895, 2.0734835, 0.016353041]\n",
            "Batch:100\n",
            "d_loss_real:1.2589571475982666\n",
            "d_loss_fake:0.2627783417701721\n",
            "d_loss_wrong:0.5182807445526123\n",
            "d_loss:0.8247433453798294\n",
            "g_loss:[1.7898074, 1.7582926, 0.015757434]\n",
            "Batch:101\n",
            "d_loss_real:1.2455649375915527\n",
            "d_loss_fake:0.12759192287921906\n",
            "d_loss_wrong:0.6364212036132812\n",
            "d_loss:0.8137857466936111\n",
            "g_loss:[1.7252603, 1.705508, 0.009876122]\n",
            "Batch:102\n",
            "d_loss_real:1.1800875663757324\n",
            "d_loss_fake:0.22832858562469482\n",
            "d_loss_wrong:0.5671288371086121\n",
            "d_loss:0.7889081388711929\n",
            "g_loss:[1.7677164, 1.7468275, 0.010444464]\n",
            "Batch:103\n",
            "d_loss_real:1.3439394235610962\n",
            "d_loss_fake:0.1499556303024292\n",
            "d_loss_wrong:0.5981074571609497\n",
            "d_loss:0.8589854836463928\n",
            "g_loss:[1.6179498, 1.5967026, 0.010623619]\n",
            "Batch:104\n",
            "d_loss_real:1.2427353858947754\n",
            "d_loss_fake:0.22339001297950745\n",
            "d_loss_wrong:0.6793208122253418\n",
            "d_loss:0.8470453917980194\n",
            "g_loss:[2.168385, 2.1453898, 0.011497574]\n",
            "Batch:105\n",
            "d_loss_real:1.1716235876083374\n",
            "d_loss_fake:0.12707355618476868\n",
            "d_loss_wrong:0.6288091540336609\n",
            "d_loss:0.7747824788093567\n",
            "g_loss:[1.9743338, 1.9573352, 0.008499291]\n",
            "Batch:106\n",
            "d_loss_real:1.1486315727233887\n",
            "d_loss_fake:0.12223590910434723\n",
            "d_loss_wrong:0.701340913772583\n",
            "d_loss:0.7802099883556366\n",
            "g_loss:[1.8385671, 1.8183107, 0.010128197]\n",
            "Batch:107\n",
            "d_loss_real:1.1635695695877075\n",
            "d_loss_fake:0.10012988746166229\n",
            "d_loss_wrong:0.6329537034034729\n",
            "d_loss:0.7650556862354279\n",
            "g_loss:[1.7832603, 1.760265, 0.011497706]\n",
            "Batch:108\n",
            "d_loss_real:1.109278678894043\n",
            "d_loss_fake:0.14671677350997925\n",
            "d_loss_wrong:0.706912636756897\n",
            "d_loss:0.7680466920137405\n",
            "g_loss:[2.0468836, 2.0274353, 0.009724118]\n",
            "Batch:109\n",
            "d_loss_real:1.1598260402679443\n",
            "d_loss_fake:0.11726592481136322\n",
            "d_loss_wrong:0.6636714339256287\n",
            "d_loss:0.7751473635435104\n",
            "g_loss:[1.8346566, 1.8135822, 0.010537181]\n",
            "Batch:110\n",
            "d_loss_real:1.1342042684555054\n",
            "d_loss_fake:0.09435674548149109\n",
            "d_loss_wrong:0.6563118100166321\n",
            "d_loss:0.7547692656517029\n",
            "g_loss:[2.1874707, 2.1619036, 0.012783501]\n",
            "Batch:111\n",
            "d_loss_real:0.9859895706176758\n",
            "d_loss_fake:0.0771506130695343\n",
            "d_loss_wrong:0.7328628301620483\n",
            "d_loss:0.695498138666153\n",
            "g_loss:[2.1024628, 2.0756733, 0.013394713]\n",
            "Batch:112\n",
            "d_loss_real:1.2072114944458008\n",
            "d_loss_fake:0.08359362930059433\n",
            "d_loss_wrong:0.7273905873298645\n",
            "d_loss:0.8063517957925797\n",
            "g_loss:[2.1705337, 2.1436372, 0.013448255]\n",
            "Batch:113\n",
            "d_loss_real:1.0365588665008545\n",
            "d_loss_fake:0.13360509276390076\n",
            "d_loss_wrong:0.6873398423194885\n",
            "d_loss:0.723515659570694\n",
            "g_loss:[2.5010695, 2.4754715, 0.012798965]\n",
            "Batch:114\n",
            "d_loss_real:1.1265931129455566\n",
            "d_loss_fake:0.056918684393167496\n",
            "d_loss_wrong:0.640446126461029\n",
            "d_loss:0.7376377582550049\n",
            "g_loss:[2.2996538, 2.2727172, 0.013468275]\n",
            "Batch:115\n",
            "d_loss_real:0.9826987981796265\n",
            "d_loss_fake:0.05610962212085724\n",
            "d_loss_wrong:0.6797245740890503\n",
            "d_loss:0.6753079444169998\n",
            "g_loss:[2.4325097, 2.4072719, 0.01261887]\n",
            "Batch:116\n",
            "d_loss_real:1.1631544828414917\n",
            "d_loss_fake:0.08813747763633728\n",
            "d_loss_wrong:0.6983315348625183\n",
            "d_loss:0.7781944870948792\n",
            "g_loss:[1.9168892, 1.895853, 0.010518065]\n",
            "Batch:117\n",
            "d_loss_real:1.0080859661102295\n",
            "d_loss_fake:0.06428161263465881\n",
            "d_loss_wrong:0.7158578038215637\n",
            "d_loss:0.699077844619751\n",
            "g_loss:[1.5240455, 1.5041152, 0.009965099]\n",
            "Batch:118\n",
            "d_loss_real:1.0247561931610107\n",
            "d_loss_fake:0.04325523599982262\n",
            "d_loss_wrong:0.6893845200538635\n",
            "d_loss:0.6955380290746689\n",
            "g_loss:[1.3049927, 1.287842, 0.008575348]\n",
            "Batch:119\n",
            "d_loss_real:1.1180591583251953\n",
            "d_loss_fake:0.03903602436184883\n",
            "d_loss_wrong:0.6369014978408813\n",
            "d_loss:0.7280139625072479\n",
            "g_loss:[1.0110295, 0.9944099, 0.008309765]\n",
            "Batch:120\n",
            "d_loss_real:0.9633123278617859\n",
            "d_loss_fake:0.04847097396850586\n",
            "d_loss_wrong:0.684660792350769\n",
            "d_loss:0.6649391055107117\n",
            "g_loss:[1.0804124, 1.0516129, 0.014399793]\n",
            "Batch:121\n",
            "d_loss_real:1.0411261320114136\n",
            "d_loss_fake:0.03865944594144821\n",
            "d_loss_wrong:0.7736464142799377\n",
            "d_loss:0.7236395329236984\n",
            "g_loss:[1.0379255, 1.0181203, 0.009902604]\n",
            "Batch:122\n",
            "d_loss_real:0.9707571268081665\n",
            "d_loss_fake:0.04944472014904022\n",
            "d_loss_wrong:0.7506172060966492\n",
            "d_loss:0.6853940486907959\n",
            "g_loss:[0.9332735, 0.9107022, 0.011285628]\n",
            "Batch:123\n",
            "d_loss_real:1.0274980068206787\n",
            "d_loss_fake:0.034100696444511414\n",
            "d_loss_wrong:0.7423356175422668\n",
            "d_loss:0.7078580856323242\n",
            "g_loss:[1.0150728, 0.9982486, 0.008412121]\n",
            "Batch:124\n",
            "d_loss_real:1.096224308013916\n",
            "d_loss_fake:0.03170175105333328\n",
            "d_loss_wrong:0.6140969395637512\n",
            "d_loss:0.709561824798584\n",
            "g_loss:[0.81384504, 0.79851127, 0.007666894]\n",
            "Batch:125\n",
            "d_loss_real:0.8880136013031006\n",
            "d_loss_fake:0.03156893700361252\n",
            "d_loss_wrong:0.6934827566146851\n",
            "d_loss:0.6252697259187698\n",
            "g_loss:[0.90762174, 0.88548076, 0.011070483]\n",
            "Batch:126\n",
            "d_loss_real:1.0755181312561035\n",
            "d_loss_fake:0.0669938176870346\n",
            "d_loss_wrong:0.7508748769760132\n",
            "d_loss:0.742226243019104\n",
            "g_loss:[0.95720065, 0.93392205, 0.011639298]\n",
            "Batch:127\n",
            "d_loss_real:1.0177395343780518\n",
            "d_loss_fake:0.07391144335269928\n",
            "d_loss_wrong:0.6367782950401306\n",
            "d_loss:0.686542198061943\n",
            "g_loss:[0.9292704, 0.9110819, 0.00909424]\n",
            "Batch:128\n",
            "d_loss_real:1.0253715515136719\n",
            "d_loss_fake:0.06985699385404587\n",
            "d_loss_wrong:0.6394096612930298\n",
            "d_loss:0.69000244140625\n",
            "g_loss:[0.86396164, 0.8329847, 0.015488477]\n",
            "Batch:129\n",
            "d_loss_real:1.0862022638320923\n",
            "d_loss_fake:0.09260142594575882\n",
            "d_loss_wrong:0.5982955098152161\n",
            "d_loss:0.7158253639936447\n",
            "g_loss:[0.9883675, 0.9569777, 0.015694877]\n",
            "Batch:130\n",
            "d_loss_real:0.9907997250556946\n",
            "d_loss_fake:0.061629846692085266\n",
            "d_loss_wrong:0.6962379813194275\n",
            "d_loss:0.6848668158054352\n",
            "g_loss:[0.94669735, 0.93153775, 0.0075797923]\n",
            "Batch:131\n",
            "d_loss_real:1.038017749786377\n",
            "d_loss_fake:0.10823498666286469\n",
            "d_loss_wrong:0.6566994190216064\n",
            "d_loss:0.7102424800395966\n",
            "g_loss:[0.8682946, 0.84296167, 0.012666473]\n",
            "Batch:132\n",
            "d_loss_real:0.9705557823181152\n",
            "d_loss_fake:0.06798762083053589\n",
            "d_loss_wrong:0.6489213109016418\n",
            "d_loss:0.664505124092102\n",
            "g_loss:[0.9708259, 0.9519495, 0.009438232]\n",
            "Batch:133\n",
            "d_loss_real:0.9908550977706909\n",
            "d_loss_fake:0.0983634889125824\n",
            "d_loss_wrong:0.6234805583953857\n",
            "d_loss:0.6758885681629181\n",
            "g_loss:[0.92378676, 0.9090804, 0.0073531866]\n",
            "Batch:134\n",
            "d_loss_real:1.0685794353485107\n",
            "d_loss_fake:0.13092589378356934\n",
            "d_loss_wrong:0.6103317141532898\n",
            "d_loss:0.7196041196584702\n",
            "g_loss:[1.1225375, 1.104708, 0.008914756]\n",
            "Batch:135\n",
            "d_loss_real:1.0737056732177734\n",
            "d_loss_fake:0.11145487427711487\n",
            "d_loss_wrong:0.5795714259147644\n",
            "d_loss:0.7096094191074371\n",
            "g_loss:[1.3104787, 1.2953882, 0.0075452584]\n",
            "Batch:136\n",
            "d_loss_real:1.0083972215652466\n",
            "d_loss_fake:0.08696231245994568\n",
            "d_loss_wrong:0.6110678315162659\n",
            "d_loss:0.6787061393260956\n",
            "g_loss:[1.0395343, 1.0271688, 0.0061827824]\n",
            "Batch:137\n",
            "d_loss_real:1.0494393110275269\n",
            "d_loss_fake:0.14165394008159637\n",
            "d_loss_wrong:0.5710614919662476\n",
            "d_loss:0.7028985172510147\n",
            "g_loss:[1.3311028, 1.3169028, 0.007100016]\n",
            "Batch:138\n",
            "d_loss_real:1.1289455890655518\n",
            "d_loss_fake:0.0895564928650856\n",
            "d_loss_wrong:0.6389172077178955\n",
            "d_loss:0.7465912252664566\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "g_loss:[1.4464982, 1.4304361, 0.008031028]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "========================================\n",
            "Epoch is: 1\n",
            "Number of batches 138\n",
            "Batch:1\n",
            "d_loss_real:1.0547401905059814\n",
            "d_loss_fake:0.10438096523284912\n",
            "d_loss_wrong:0.6689687371253967\n",
            "d_loss:0.7207075208425522\n",
            "g_loss:[1.0579097, 1.0402124, 0.008848677]\n",
            "Batch:2\n",
            "d_loss_real:1.0134689807891846\n",
            "d_loss_fake:0.09892016649246216\n",
            "d_loss_wrong:0.6601747870445251\n",
            "d_loss:0.6965082287788391\n",
            "g_loss:[1.0689243, 1.0471127, 0.010905802]\n",
            "Batch:3\n",
            "d_loss_real:1.0248138904571533\n",
            "d_loss_fake:0.05202134698629379\n",
            "d_loss_wrong:0.6581671833992004\n",
            "d_loss:0.6899540722370148\n",
            "g_loss:[1.0187626, 0.9912105, 0.013776045]\n",
            "Batch:4\n",
            "d_loss_real:1.1995162963867188\n",
            "d_loss_fake:0.0840444266796112\n",
            "d_loss_wrong:0.7010015845298767\n",
            "d_loss:0.7960196435451508\n",
            "g_loss:[0.92627627, 0.8982581, 0.014009074]\n",
            "Batch:5\n",
            "d_loss_real:1.0475256443023682\n",
            "d_loss_fake:0.09673856943845749\n",
            "d_loss_wrong:0.7073392868041992\n",
            "d_loss:0.7247822880744934\n",
            "g_loss:[1.032878, 1.0100279, 0.011425049]\n",
            "Batch:6\n",
            "d_loss_real:1.1832102537155151\n",
            "d_loss_fake:0.10340883582830429\n",
            "d_loss_wrong:0.6920958757400513\n",
            "d_loss:0.790481299161911\n",
            "g_loss:[0.8989895, 0.87839156, 0.010298973]\n",
            "Batch:7\n",
            "d_loss_real:0.9914901852607727\n",
            "d_loss_fake:0.13423572480678558\n",
            "d_loss_wrong:0.6641072630882263\n",
            "d_loss:0.6953308433294296\n",
            "g_loss:[1.0112737, 0.9948193, 0.008227257]\n",
            "Batch:8\n",
            "d_loss_real:1.1169626712799072\n",
            "d_loss_fake:0.12099723517894745\n",
            "d_loss_wrong:0.5653505921363831\n",
            "d_loss:0.7300682961940765\n",
            "g_loss:[1.0851786, 1.0605417, 0.012318447]\n",
            "Batch:9\n",
            "d_loss_real:1.0523715019226074\n",
            "d_loss_fake:0.0930597186088562\n",
            "d_loss_wrong:0.6937609314918518\n",
            "d_loss:0.7228909134864807\n",
            "g_loss:[1.1729612, 1.1388195, 0.017070912]\n",
            "Batch:10\n",
            "d_loss_real:1.0401244163513184\n",
            "d_loss_fake:0.07180604338645935\n",
            "d_loss_wrong:0.6772225499153137\n",
            "d_loss:0.7073193490505219\n",
            "g_loss:[1.1436774, 1.1142793, 0.014699042]\n",
            "Batch:11\n",
            "d_loss_real:1.2088115215301514\n",
            "d_loss_fake:0.08598117530345917\n",
            "d_loss_wrong:0.5904627442359924\n",
            "d_loss:0.7735167443752289\n",
            "g_loss:[1.0487416, 1.031899, 0.008421309]\n",
            "Batch:12\n",
            "d_loss_real:1.1302846670150757\n",
            "d_loss_fake:0.1463412344455719\n",
            "d_loss_wrong:0.6009482741355896\n",
            "d_loss:0.7519647181034088\n",
            "g_loss:[0.93795824, 0.9222915, 0.007833368]\n",
            "Batch:13\n",
            "d_loss_real:1.071894884109497\n",
            "d_loss_fake:0.12335763871669769\n",
            "d_loss_wrong:0.6060759425163269\n",
            "d_loss:0.718305841088295\n",
            "g_loss:[1.1036057, 1.0892804, 0.0071627037]\n",
            "Batch:14\n",
            "d_loss_real:1.2176927328109741\n",
            "d_loss_fake:0.21371805667877197\n",
            "d_loss_wrong:0.5635918378829956\n",
            "d_loss:0.803173840045929\n",
            "g_loss:[0.9860361, 0.97291756, 0.0065592863]\n",
            "Batch:15\n",
            "d_loss_real:1.0005860328674316\n",
            "d_loss_fake:0.08706846833229065\n",
            "d_loss_wrong:0.7398595213890076\n",
            "d_loss:0.707025021314621\n",
            "g_loss:[1.1020383, 1.0722088, 0.014914744]\n",
            "Batch:16\n",
            "d_loss_real:0.9683600068092346\n",
            "d_loss_fake:0.11021608114242554\n",
            "d_loss_wrong:0.7130517959594727\n",
            "d_loss:0.6899969726800919\n",
            "g_loss:[1.2049721, 1.1815766, 0.0116977645]\n",
            "Batch:17\n",
            "d_loss_real:1.338693618774414\n",
            "d_loss_fake:0.17021602392196655\n",
            "d_loss_wrong:0.6718822717666626\n",
            "d_loss:0.8798713833093643\n",
            "g_loss:[0.99405617, 0.9704185, 0.0118188225]\n",
            "Batch:18\n",
            "d_loss_real:1.0370155572891235\n",
            "d_loss_fake:0.280603289604187\n",
            "d_loss_wrong:0.607354462146759\n",
            "d_loss:0.7404972165822983\n",
            "g_loss:[1.4131432, 1.3980885, 0.00752736]\n",
            "Batch:19\n",
            "d_loss_real:1.1907095909118652\n",
            "d_loss_fake:0.09734898805618286\n",
            "d_loss_wrong:0.5988958477973938\n",
            "d_loss:0.7694160044193268\n",
            "g_loss:[1.3601454, 1.3461897, 0.006977828]\n",
            "Batch:20\n",
            "d_loss_real:1.4274848699569702\n",
            "d_loss_fake:0.13748809695243835\n",
            "d_loss_wrong:0.6141276955604553\n",
            "d_loss:0.9016463756561279\n",
            "g_loss:[1.0193489, 1.003341, 0.008003937]\n",
            "Batch:21\n",
            "d_loss_real:1.0894112586975098\n",
            "d_loss_fake:0.15989434719085693\n",
            "d_loss_wrong:0.604036271572113\n",
            "d_loss:0.7356882840394974\n",
            "g_loss:[1.0426338, 1.0310627, 0.0057855146]\n",
            "Batch:22\n",
            "d_loss_real:1.111713171005249\n",
            "d_loss_fake:0.1668454110622406\n",
            "d_loss_wrong:0.6360822319984436\n",
            "d_loss:0.756588488817215\n",
            "g_loss:[1.2437074, 1.2340214, 0.004842976]\n",
            "Batch:23\n",
            "d_loss_real:1.1074028015136719\n",
            "d_loss_fake:0.11234278976917267\n",
            "d_loss_wrong:0.6776348352432251\n",
            "d_loss:0.7511958032846451\n",
            "g_loss:[0.9942307, 0.98213893, 0.006045873]\n",
            "Batch:24\n",
            "d_loss_real:0.8919416666030884\n",
            "d_loss_fake:0.07941332459449768\n",
            "d_loss_wrong:0.7376645803451538\n",
            "d_loss:0.6502403020858765\n",
            "g_loss:[1.1987238, 1.1798029, 0.009460434]\n",
            "Batch:25\n",
            "d_loss_real:1.145822525024414\n",
            "d_loss_fake:0.10034394264221191\n",
            "d_loss_wrong:0.6987839341163635\n",
            "d_loss:0.7726932317018509\n",
            "g_loss:[1.25499, 1.2382879, 0.008351003]\n",
            "Batch:26\n",
            "d_loss_real:0.9894877672195435\n",
            "d_loss_fake:0.09499076753854752\n",
            "d_loss_wrong:0.7299305200576782\n",
            "d_loss:0.7009742110967636\n",
            "g_loss:[1.4855897, 1.472723, 0.0064333603]\n",
            "Batch:27\n",
            "d_loss_real:1.369255542755127\n",
            "d_loss_fake:0.07544874399900436\n",
            "d_loss_wrong:0.6684364676475525\n",
            "d_loss:0.8705990761518478\n",
            "g_loss:[1.0907, 1.0754932, 0.00760343]\n",
            "Batch:28\n",
            "d_loss_real:1.3087432384490967\n",
            "d_loss_fake:0.14752298593521118\n",
            "d_loss_wrong:0.8967829942703247\n",
            "d_loss:0.9154481291770935\n",
            "g_loss:[1.121343, 1.1032715, 0.009035779]\n",
            "Batch:29\n",
            "d_loss_real:1.067328929901123\n",
            "d_loss_fake:0.13603702187538147\n",
            "d_loss_wrong:0.5695496797561646\n",
            "d_loss:0.7100611329078674\n",
            "g_loss:[1.2892408, 1.2715747, 0.008833062]\n",
            "Batch:30\n",
            "d_loss_real:1.170844554901123\n",
            "d_loss_fake:0.18190713226795197\n",
            "d_loss_wrong:0.8148970007896423\n",
            "d_loss:0.8346233069896698\n",
            "g_loss:[1.3393247, 1.3216214, 0.008851667]\n",
            "Batch:31\n",
            "d_loss_real:1.0810880661010742\n",
            "d_loss_fake:0.1766286939382553\n",
            "d_loss_wrong:0.5685181617736816\n",
            "d_loss:0.7268307507038116\n",
            "g_loss:[1.2876513, 1.2724378, 0.007606764]\n",
            "Batch:32\n",
            "d_loss_real:1.3059191703796387\n",
            "d_loss_fake:0.20880991220474243\n",
            "d_loss_wrong:0.5496161580085754\n",
            "d_loss:0.8425661027431488\n",
            "g_loss:[1.1153817, 1.0952241, 0.010078773]\n",
            "Batch:33\n",
            "d_loss_real:1.1888351440429688\n",
            "d_loss_fake:0.22456923127174377\n",
            "d_loss_wrong:0.5500614643096924\n",
            "d_loss:0.7880752384662628\n",
            "g_loss:[1.0513328, 1.034898, 0.008217399]\n",
            "Batch:34\n",
            "d_loss_real:1.177107572555542\n",
            "d_loss_fake:0.2774147391319275\n",
            "d_loss_wrong:0.5494585037231445\n",
            "d_loss:0.795272096991539\n",
            "g_loss:[1.2410175, 1.2239335, 0.008542006]\n",
            "Batch:35\n",
            "d_loss_real:1.170373797416687\n",
            "d_loss_fake:0.2069120556116104\n",
            "d_loss_wrong:0.5596643090248108\n",
            "d_loss:0.7768309861421585\n",
            "g_loss:[1.3382229, 1.3230042, 0.0076092784]\n",
            "Batch:36\n",
            "d_loss_real:1.0041499137878418\n",
            "d_loss_fake:0.13009849190711975\n",
            "d_loss_wrong:0.6401965022087097\n",
            "d_loss:0.6946487128734589\n",
            "g_loss:[1.6791958, 1.6603673, 0.009414244]\n",
            "Batch:37\n",
            "d_loss_real:1.0504084825515747\n",
            "d_loss_fake:0.11703193187713623\n",
            "d_loss_wrong:0.6323545575141907\n",
            "d_loss:0.7125508636236191\n",
            "g_loss:[1.485756, 1.469397, 0.008179562]\n",
            "Batch:38\n",
            "d_loss_real:1.0275514125823975\n",
            "d_loss_fake:0.172090083360672\n",
            "d_loss_wrong:0.6318343281745911\n",
            "d_loss:0.7147568166255951\n",
            "g_loss:[1.4048681, 1.3883148, 0.008276627]\n",
            "Batch:39\n",
            "d_loss_real:1.05198335647583\n",
            "d_loss_fake:0.19481992721557617\n",
            "d_loss_wrong:0.6010484099388123\n",
            "d_loss:0.7249587625265121\n",
            "g_loss:[1.2945417, 1.2791097, 0.0077159777]\n",
            "Batch:40\n",
            "d_loss_real:1.1381244659423828\n",
            "d_loss_fake:0.1675596982240677\n",
            "d_loss_wrong:0.6966978907585144\n",
            "d_loss:0.7851266264915466\n",
            "g_loss:[1.3803601, 1.3689507, 0.0057046795]\n",
            "Batch:41\n",
            "d_loss_real:1.097397804260254\n",
            "d_loss_fake:0.11872074007987976\n",
            "d_loss_wrong:0.5718482732772827\n",
            "d_loss:0.7213411629199982\n",
            "g_loss:[1.174137, 1.161009, 0.006564006]\n",
            "Batch:42\n",
            "d_loss_real:1.2527295351028442\n",
            "d_loss_fake:0.09929285943508148\n",
            "d_loss_wrong:0.5585351586341858\n",
            "d_loss:0.7908217757940292\n",
            "g_loss:[1.1498331, 1.1366577, 0.0065876828]\n",
            "Batch:43\n",
            "d_loss_real:1.4509873390197754\n",
            "d_loss_fake:0.24883411824703217\n",
            "d_loss_wrong:0.5527241230010986\n",
            "d_loss:0.9258832335472107\n",
            "g_loss:[1.0938122, 1.0804173, 0.006697469]\n",
            "Batch:44\n",
            "d_loss_real:1.095058560371399\n",
            "d_loss_fake:0.2537596523761749\n",
            "d_loss_wrong:0.5787129402160645\n",
            "d_loss:0.7556474208831787\n",
            "g_loss:[1.2805768, 1.2686678, 0.005954504]\n",
            "Batch:45\n",
            "d_loss_real:1.1358853578567505\n",
            "d_loss_fake:0.07345110177993774\n",
            "d_loss_wrong:0.506548285484314\n",
            "d_loss:0.7129425257444382\n",
            "g_loss:[1.3275968, 1.313114, 0.0072413804]\n",
            "Batch:46\n",
            "d_loss_real:0.9608199596405029\n",
            "d_loss_fake:0.08751744031906128\n",
            "d_loss_wrong:0.6650213003158569\n",
            "d_loss:0.668544664978981\n",
            "g_loss:[1.2013148, 1.1872025, 0.007056157]\n",
            "Batch:47\n",
            "d_loss_real:0.9793145060539246\n",
            "d_loss_fake:0.09074214100837708\n",
            "d_loss_wrong:0.6453753709793091\n",
            "d_loss:0.6736866235733032\n",
            "g_loss:[1.1713862, 1.1589212, 0.0062324964]\n",
            "Batch:48\n",
            "d_loss_real:1.1874656677246094\n",
            "d_loss_fake:0.163753479719162\n",
            "d_loss_wrong:0.5807033181190491\n",
            "d_loss:0.7798470258712769\n",
            "g_loss:[1.1096466, 1.0956385, 0.007004031]\n",
            "Batch:49\n",
            "d_loss_real:1.1442039012908936\n",
            "d_loss_fake:0.23500993847846985\n",
            "d_loss_wrong:0.497161865234375\n",
            "d_loss:0.7551448941230774\n",
            "g_loss:[1.1892123, 1.1747391, 0.0072365827]\n",
            "Batch:50\n",
            "d_loss_real:1.0743411779403687\n",
            "d_loss_fake:0.16390278935432434\n",
            "d_loss_wrong:0.5521135330200195\n",
            "d_loss:0.7161746621131897\n",
            "g_loss:[1.1677401, 1.157222, 0.005259033]\n",
            "Batch:51\n",
            "d_loss_real:1.174926519393921\n",
            "d_loss_fake:0.13653762638568878\n",
            "d_loss_wrong:0.6290876865386963\n",
            "d_loss:0.7788695842027664\n",
            "g_loss:[1.2184603, 1.2093277, 0.0045663]\n",
            "Batch:52\n",
            "d_loss_real:1.093835473060608\n",
            "d_loss_fake:0.2139047384262085\n",
            "d_loss_wrong:0.6175102591514587\n",
            "d_loss:0.7547714859247208\n",
            "g_loss:[1.1411991, 1.1330948, 0.0040521407]\n",
            "Batch:53\n",
            "d_loss_real:1.2555227279663086\n",
            "d_loss_fake:0.15401269495487213\n",
            "d_loss_wrong:0.7055229544639587\n",
            "d_loss:0.8426452726125717\n",
            "g_loss:[1.1667396, 1.1553404, 0.0056995936]\n",
            "Batch:54\n",
            "d_loss_real:0.8608734011650085\n",
            "d_loss_fake:0.20406612753868103\n",
            "d_loss_wrong:0.6785039901733398\n",
            "d_loss:0.6510792374610901\n",
            "g_loss:[1.8060104, 1.7962937, 0.0048582866]\n",
            "Batch:55\n",
            "d_loss_real:1.1192001104354858\n",
            "d_loss_fake:0.08728771656751633\n",
            "d_loss_wrong:0.5540966987609863\n",
            "d_loss:0.7199461609125137\n",
            "g_loss:[1.438288, 1.4264464, 0.005920776]\n",
            "Batch:56\n",
            "d_loss_real:1.2816200256347656\n",
            "d_loss_fake:0.1235933005809784\n",
            "d_loss_wrong:0.6015405654907227\n",
            "d_loss:0.8220934867858887\n",
            "g_loss:[1.646623, 1.6341279, 0.006247578]\n",
            "Batch:57\n",
            "d_loss_real:1.0559202432632446\n",
            "d_loss_fake:0.1859879493713379\n",
            "d_loss_wrong:0.6918870210647583\n",
            "d_loss:0.7474288642406464\n",
            "g_loss:[1.6075823, 1.5917076, 0.007937342]\n",
            "Batch:58\n",
            "d_loss_real:1.0157225131988525\n",
            "d_loss_fake:0.10352297127246857\n",
            "d_loss_wrong:0.6361947655677795\n",
            "d_loss:0.692790687084198\n",
            "g_loss:[1.5905902, 1.5798416, 0.00537431]\n",
            "Batch:59\n",
            "d_loss_real:1.0537312030792236\n",
            "d_loss_fake:0.10590295493602753\n",
            "d_loss_wrong:0.731540858745575\n",
            "d_loss:0.7362265586853027\n",
            "g_loss:[2.011497, 1.9963627, 0.0075672064]\n",
            "Batch:60\n",
            "d_loss_real:1.0274357795715332\n",
            "d_loss_fake:0.09738368541002274\n",
            "d_loss_wrong:0.6520864367485046\n",
            "d_loss:0.7010854184627533\n",
            "g_loss:[2.0243778, 2.0093913, 0.0074932305]\n",
            "Batch:61\n",
            "d_loss_real:1.1473053693771362\n",
            "d_loss_fake:0.06843727827072144\n",
            "d_loss_wrong:0.6287853717803955\n",
            "d_loss:0.7479583472013474\n",
            "g_loss:[1.6541309, 1.6440287, 0.005051108]\n",
            "Batch:62\n",
            "d_loss_real:1.2712523937225342\n",
            "d_loss_fake:0.05315258353948593\n",
            "d_loss_wrong:0.6685542464256287\n",
            "d_loss:0.8160528987646103\n",
            "g_loss:[1.4485803, 1.4386251, 0.0049775993]\n",
            "Batch:63\n",
            "d_loss_real:1.15398371219635\n",
            "d_loss_fake:0.07700319588184357\n",
            "d_loss_wrong:0.6039125919342041\n",
            "d_loss:0.7472207993268967\n",
            "g_loss:[1.4251156, 1.413236, 0.005939778]\n",
            "Batch:64\n",
            "d_loss_real:1.0632095336914062\n",
            "d_loss_fake:0.17953269183635712\n",
            "d_loss_wrong:0.5888031721115112\n",
            "d_loss:0.7236887365579605\n",
            "g_loss:[1.3910669, 1.3814445, 0.0048112264]\n",
            "Batch:65\n",
            "d_loss_real:1.0727336406707764\n",
            "d_loss_fake:0.10536856949329376\n",
            "d_loss_wrong:0.6391850709915161\n",
            "d_loss:0.7225052267313004\n",
            "g_loss:[1.3602884, 1.3460803, 0.007104044]\n",
            "Batch:66\n",
            "d_loss_real:0.96368008852005\n",
            "d_loss_fake:0.09447579830884933\n",
            "d_loss_wrong:0.6248477101325989\n",
            "d_loss:0.6616709232330322\n",
            "g_loss:[1.3486401, 1.3401415, 0.0042492566]\n",
            "Batch:67\n",
            "d_loss_real:1.1893832683563232\n",
            "d_loss_fake:0.11045505106449127\n",
            "d_loss_wrong:0.64679354429245\n",
            "d_loss:0.7840037792921066\n",
            "g_loss:[1.2698181, 1.260031, 0.004893532]\n",
            "Batch:68\n",
            "d_loss_real:0.9368471503257751\n",
            "d_loss_fake:0.13996949791908264\n",
            "d_loss_wrong:0.718966543674469\n",
            "d_loss:0.6831575930118561\n",
            "g_loss:[1.3652334, 1.355411, 0.004911162]\n",
            "Batch:69\n",
            "d_loss_real:0.989678144454956\n",
            "d_loss_fake:0.07020626217126846\n",
            "d_loss_wrong:0.7859803438186646\n",
            "d_loss:0.7088857293128967\n",
            "g_loss:[1.5289215, 1.5175321, 0.00569469]\n",
            "Batch:70\n",
            "d_loss_real:1.0983935594558716\n",
            "d_loss_fake:0.10076868534088135\n",
            "d_loss_wrong:0.6135735511779785\n",
            "d_loss:0.7277823388576508\n",
            "g_loss:[1.6899948, 1.6801907, 0.004902048]\n",
            "Batch:71\n",
            "d_loss_real:1.1973750591278076\n",
            "d_loss_fake:0.06409423053264618\n",
            "d_loss_wrong:0.533072829246521\n",
            "d_loss:0.7479792982339859\n",
            "g_loss:[1.7720573, 1.7594613, 0.006298025]\n",
            "Batch:72\n",
            "d_loss_real:1.0254995822906494\n",
            "d_loss_fake:0.07038498669862747\n",
            "d_loss_wrong:0.6879616379737854\n",
            "d_loss:0.7023364454507828\n",
            "g_loss:[1.5743567, 1.5659621, 0.004197275]\n",
            "Batch:73\n",
            "d_loss_real:1.041832447052002\n",
            "d_loss_fake:0.08093912154436111\n",
            "d_loss_wrong:0.6690110564231873\n",
            "d_loss:0.7084037661552429\n",
            "g_loss:[1.656759, 1.6485862, 0.004086455]\n",
            "Batch:74\n",
            "d_loss_real:1.0130980014801025\n",
            "d_loss_fake:0.1454763114452362\n",
            "d_loss_wrong:0.6050827503204346\n",
            "d_loss:0.6941887736320496\n",
            "g_loss:[1.6316271, 1.6237324, 0.0039473227]\n",
            "Batch:75\n",
            "d_loss_real:1.0982271432876587\n",
            "d_loss_fake:0.089789979159832\n",
            "d_loss_wrong:0.6653290390968323\n",
            "d_loss:0.7378933280706406\n",
            "g_loss:[1.5472237, 1.5376639, 0.0047798953]\n",
            "Batch:76\n",
            "d_loss_real:1.0620088577270508\n",
            "d_loss_fake:0.055720701813697815\n",
            "d_loss_wrong:0.657498300075531\n",
            "d_loss:0.7093091756105423\n",
            "g_loss:[1.4196618, 1.4113108, 0.004175497]\n",
            "Batch:77\n",
            "d_loss_real:0.9173136353492737\n",
            "d_loss_fake:0.05138740688562393\n",
            "d_loss_wrong:0.7174662947654724\n",
            "d_loss:0.6508702486753464\n",
            "g_loss:[1.5001193, 1.4918799, 0.0041196775]\n",
            "Batch:78\n",
            "d_loss_real:0.9465377926826477\n",
            "d_loss_fake:0.0662330612540245\n",
            "d_loss_wrong:0.6959221363067627\n",
            "d_loss:0.6638076901435852\n",
            "g_loss:[1.8578098, 1.84988, 0.0039648796]\n",
            "Batch:79\n",
            "d_loss_real:1.0519390106201172\n",
            "d_loss_fake:0.07667648047208786\n",
            "d_loss_wrong:0.5979655981063843\n",
            "d_loss:0.6946300268173218\n",
            "g_loss:[1.6542685, 1.6463752, 0.003946669]\n",
            "Batch:80\n",
            "d_loss_real:1.0582702159881592\n",
            "d_loss_fake:0.061919085681438446\n",
            "d_loss_wrong:0.6327767968177795\n",
            "d_loss:0.7028090804815292\n",
            "g_loss:[1.8214127, 1.813814, 0.0037993318]\n",
            "Batch:81\n",
            "d_loss_real:1.0038411617279053\n",
            "d_loss_fake:0.04212694615125656\n",
            "d_loss_wrong:0.6031753420829773\n",
            "d_loss:0.6632461547851562\n",
            "g_loss:[1.6386538, 1.6296284, 0.0045126565]\n",
            "Batch:82\n",
            "d_loss_real:0.9907265901565552\n",
            "d_loss_fake:0.02672545425593853\n",
            "d_loss_wrong:0.6651748418807983\n",
            "d_loss:0.6683383733034134\n",
            "g_loss:[1.3734783, 1.3650696, 0.00420436]\n",
            "Batch:83\n",
            "d_loss_real:0.9180992841720581\n",
            "d_loss_fake:0.055139824748039246\n",
            "d_loss_wrong:0.6955713033676147\n",
            "d_loss:0.6467274278402328\n",
            "g_loss:[1.6046879, 1.595273, 0.0047074817]\n",
            "Batch:84\n",
            "d_loss_real:0.9356430768966675\n",
            "d_loss_fake:0.02580656111240387\n",
            "d_loss_wrong:0.7618823647499084\n",
            "d_loss:0.6647437661886215\n",
            "g_loss:[1.2353461, 1.2256542, 0.0048459424]\n",
            "Batch:85\n",
            "d_loss_real:0.8905677795410156\n",
            "d_loss_fake:0.01877775229513645\n",
            "d_loss_wrong:0.6781589388847351\n",
            "d_loss:0.619518056511879\n",
            "g_loss:[1.2119538, 1.2039428, 0.0040054885]\n",
            "Batch:86\n",
            "d_loss_real:0.9263426065444946\n",
            "d_loss_fake:0.010917376726865768\n",
            "d_loss_wrong:0.6926698088645935\n",
            "d_loss:0.6390680968761444\n",
            "g_loss:[1.2240177, 1.2157888, 0.0041144285]\n",
            "Batch:87\n",
            "d_loss_real:0.9335051774978638\n",
            "d_loss_fake:0.021432161331176758\n",
            "d_loss_wrong:0.6528466939926147\n",
            "d_loss:0.6353223025798798\n",
            "g_loss:[1.128347, 1.1170092, 0.00566891]\n",
            "Batch:88\n",
            "d_loss_real:1.011349081993103\n",
            "d_loss_fake:0.012047817930579185\n",
            "d_loss_wrong:0.6652173399925232\n",
            "d_loss:0.6749908328056335\n",
            "g_loss:[1.152764, 1.1420412, 0.0053613726]\n",
            "Batch:89\n",
            "d_loss_real:1.11683988571167\n",
            "d_loss_fake:0.041636720299720764\n",
            "d_loss_wrong:0.9822667241096497\n",
            "d_loss:0.8143958151340485\n",
            "g_loss:[0.9538858, 0.9400073, 0.006939225]\n",
            "Batch:90\n",
            "d_loss_real:0.8698039650917053\n",
            "d_loss_fake:0.029608068987727165\n",
            "d_loss_wrong:0.7262386679649353\n",
            "d_loss:0.6238636672496796\n",
            "g_loss:[1.0849799, 1.0749251, 0.0050274404]\n",
            "Batch:91\n",
            "d_loss_real:0.9620833396911621\n",
            "d_loss_fake:0.017175549641251564\n",
            "d_loss_wrong:0.702341616153717\n",
            "d_loss:0.6609209626913071\n",
            "g_loss:[1.1170497, 1.1084368, 0.0043064086]\n",
            "Batch:92\n",
            "d_loss_real:1.273991346359253\n",
            "d_loss_fake:0.06801721453666687\n",
            "d_loss_wrong:1.1084660291671753\n",
            "d_loss:0.9311164915561676\n",
            "g_loss:[0.9717065, 0.95533264, 0.008186949]\n",
            "Batch:93\n",
            "d_loss_real:1.076583743095398\n",
            "d_loss_fake:0.04450664669275284\n",
            "d_loss_wrong:0.6182886362075806\n",
            "d_loss:0.7039906978607178\n",
            "g_loss:[1.1969782, 1.1806505, 0.008163866]\n",
            "Batch:94\n",
            "d_loss_real:0.9147335290908813\n",
            "d_loss_fake:0.05289639160037041\n",
            "d_loss_wrong:0.6653909087181091\n",
            "d_loss:0.6369385868310928\n",
            "g_loss:[0.9986643, 0.98564065, 0.0065118447]\n",
            "Batch:95\n",
            "d_loss_real:0.9134417772293091\n",
            "d_loss_fake:0.032260432839393616\n",
            "d_loss_wrong:0.6579645872116089\n",
            "d_loss:0.6292771399021149\n",
            "g_loss:[1.012558, 1.0007303, 0.0059138457]\n",
            "Batch:96\n",
            "d_loss_real:0.8118295073509216\n",
            "d_loss_fake:0.027131423354148865\n",
            "d_loss_wrong:0.6564911007881165\n",
            "d_loss:0.5768203884363174\n",
            "g_loss:[1.0614233, 1.0482773, 0.0065730475]\n",
            "Batch:97\n",
            "d_loss_real:0.9395749568939209\n",
            "d_loss_fake:0.05032944679260254\n",
            "d_loss_wrong:0.6294629573822021\n",
            "d_loss:0.6397355794906616\n",
            "g_loss:[1.141703, 1.1284003, 0.006651361]\n",
            "Batch:98\n",
            "d_loss_real:0.8733339905738831\n",
            "d_loss_fake:0.04749767482280731\n",
            "d_loss_wrong:0.6893012523651123\n",
            "d_loss:0.6208667308092117\n",
            "g_loss:[1.1538423, 1.1418529, 0.0059947576]\n",
            "Batch:99\n",
            "d_loss_real:1.1789369583129883\n",
            "d_loss_fake:0.05586343631148338\n",
            "d_loss_wrong:0.5603453516960144\n",
            "d_loss:0.7435206770896912\n",
            "g_loss:[0.9425768, 0.9323088, 0.0051340256]\n",
            "Batch:100\n",
            "d_loss_real:1.097893238067627\n",
            "d_loss_fake:0.10106596350669861\n",
            "d_loss_wrong:0.5695690512657166\n",
            "d_loss:0.7166053652763367\n",
            "g_loss:[0.9345385, 0.9238119, 0.005363288]\n",
            "Batch:101\n",
            "d_loss_real:0.9246243238449097\n",
            "d_loss_fake:0.07304802536964417\n",
            "d_loss_wrong:0.6245061755180359\n",
            "d_loss:0.6367007195949554\n",
            "g_loss:[0.954817, 0.947852, 0.0034824908]\n",
            "Batch:102\n",
            "d_loss_real:1.2199050188064575\n",
            "d_loss_fake:0.04785548150539398\n",
            "d_loss_wrong:0.623154878616333\n",
            "d_loss:0.7777051031589508\n",
            "g_loss:[0.80781764, 0.80124676, 0.0032854425]\n",
            "Batch:103\n",
            "d_loss_real:1.081843376159668\n",
            "d_loss_fake:0.04447944834828377\n",
            "d_loss_wrong:0.6205372214317322\n",
            "d_loss:0.7071758508682251\n",
            "g_loss:[0.76655334, 0.7604146, 0.003069363]\n",
            "Batch:104\n",
            "d_loss_real:1.2426526546478271\n",
            "d_loss_fake:0.10597500205039978\n",
            "d_loss_wrong:0.616578221321106\n",
            "d_loss:0.8019646406173706\n",
            "g_loss:[0.752689, 0.74613273, 0.0032781432]\n",
            "Batch:105\n",
            "d_loss_real:0.9193292260169983\n",
            "d_loss_fake:0.08898375183343887\n",
            "d_loss_wrong:0.7048198580741882\n",
            "d_loss:0.6581155210733414\n",
            "g_loss:[0.79523796, 0.7898586, 0.0026897]\n",
            "Batch:106\n",
            "d_loss_real:0.9059436321258545\n",
            "d_loss_fake:0.02809903211891651\n",
            "d_loss_wrong:0.6613776087760925\n",
            "d_loss:0.6253409832715988\n",
            "g_loss:[0.8291255, 0.82307833, 0.0030236053]\n",
            "Batch:107\n",
            "d_loss_real:0.9932793378829956\n",
            "d_loss_fake:0.024929417297244072\n",
            "d_loss_wrong:0.7132354378700256\n",
            "d_loss:0.6811808794736862\n",
            "g_loss:[0.7738274, 0.76716405, 0.003331659]\n",
            "Batch:108\n",
            "d_loss_real:1.119981288909912\n",
            "d_loss_fake:0.0625913143157959\n",
            "d_loss_wrong:0.6980249285697937\n",
            "d_loss:0.7501447051763535\n",
            "g_loss:[0.7161702, 0.71040046, 0.002884877]\n",
            "Batch:109\n",
            "d_loss_real:0.9235856533050537\n",
            "d_loss_fake:0.04878968000411987\n",
            "d_loss_wrong:0.7055870890617371\n",
            "d_loss:0.6503870189189911\n",
            "g_loss:[0.75308126, 0.7466997, 0.003190796]\n",
            "Batch:110\n",
            "d_loss_real:0.9826814532279968\n",
            "d_loss_fake:0.055000923573970795\n",
            "d_loss_wrong:0.648747444152832\n",
            "d_loss:0.6672778129577637\n",
            "g_loss:[0.8275041, 0.820228, 0.0036380584]\n",
            "Batch:111\n",
            "d_loss_real:1.127044916152954\n",
            "d_loss_fake:0.0382356271147728\n",
            "d_loss_wrong:0.7014641165733337\n",
            "d_loss:0.7484473884105682\n",
            "g_loss:[0.74368376, 0.7357704, 0.003956686]\n",
            "Batch:112\n",
            "d_loss_real:1.0339453220367432\n",
            "d_loss_fake:0.07010155916213989\n",
            "d_loss_wrong:0.6883025765419006\n",
            "d_loss:0.7065736949443817\n",
            "g_loss:[0.7726678, 0.7652191, 0.003724358]\n",
            "Batch:113\n",
            "d_loss_real:1.0115032196044922\n",
            "d_loss_fake:0.030321916565299034\n",
            "d_loss_wrong:0.7121411561965942\n",
            "d_loss:0.6913673728704453\n",
            "g_loss:[0.7680831, 0.7611305, 0.0034762982]\n",
            "Batch:114\n",
            "d_loss_real:0.8933382034301758\n",
            "d_loss_fake:0.026413384824991226\n",
            "d_loss_wrong:0.7151896357536316\n",
            "d_loss:0.632069855928421\n",
            "g_loss:[0.7472534, 0.73935604, 0.0039487025]\n",
            "Batch:115\n",
            "d_loss_real:0.83870530128479\n",
            "d_loss_fake:0.03333533555269241\n",
            "d_loss_wrong:0.6895873546600342\n",
            "d_loss:0.6000833213329315\n",
            "g_loss:[0.7882918, 0.78108215, 0.0036048428]\n",
            "Batch:116\n",
            "d_loss_real:1.0257519483566284\n",
            "d_loss_fake:0.08274366706609726\n",
            "d_loss_wrong:0.6489738821983337\n",
            "d_loss:0.6958053559064865\n",
            "g_loss:[0.83693683, 0.83021116, 0.0033628494]\n",
            "Batch:117\n",
            "d_loss_real:0.9833930730819702\n",
            "d_loss_fake:0.09719815850257874\n",
            "d_loss_wrong:0.6706758141517639\n",
            "d_loss:0.6836650371551514\n",
            "g_loss:[0.8426524, 0.83641446, 0.00311896]\n",
            "Batch:118\n",
            "d_loss_real:1.0298917293548584\n",
            "d_loss_fake:0.07967383414506912\n",
            "d_loss_wrong:0.6146800518035889\n",
            "d_loss:0.6885343343019485\n",
            "g_loss:[0.88484097, 0.87918293, 0.002829013]\n",
            "Batch:119\n",
            "d_loss_real:1.1185270547866821\n",
            "d_loss_fake:0.12337910383939743\n",
            "d_loss_wrong:0.6494383811950684\n",
            "d_loss:0.7524679005146027\n",
            "g_loss:[0.88225985, 0.87678283, 0.0027385133]\n",
            "Batch:120\n",
            "d_loss_real:0.8955111503601074\n",
            "d_loss_fake:0.14010897278785706\n",
            "d_loss_wrong:0.6432130932807922\n",
            "d_loss:0.6435860991477966\n",
            "g_loss:[1.094536, 1.0859516, 0.004292194]\n",
            "Batch:121\n",
            "d_loss_real:1.2792081832885742\n",
            "d_loss_fake:0.20167218148708344\n",
            "d_loss_wrong:0.6189089417457581\n",
            "d_loss:0.8447493761777878\n",
            "g_loss:[0.9281463, 0.92184615, 0.0031500864]\n",
            "Batch:122\n",
            "d_loss_real:1.1281366348266602\n",
            "d_loss_fake:0.15423119068145752\n",
            "d_loss_wrong:0.6363968253135681\n",
            "d_loss:0.7617253214120865\n",
            "g_loss:[0.9943221, 0.9870548, 0.0036336572]\n",
            "Batch:123\n",
            "d_loss_real:1.2050790786743164\n",
            "d_loss_fake:0.11044484376907349\n",
            "d_loss_wrong:0.6311303377151489\n",
            "d_loss:0.7879333347082138\n",
            "g_loss:[1.039947, 1.0339797, 0.0029837177]\n",
            "Batch:124\n",
            "d_loss_real:1.0659377574920654\n",
            "d_loss_fake:0.06209824979305267\n",
            "d_loss_wrong:0.630487322807312\n",
            "d_loss:0.7061152756214142\n",
            "g_loss:[1.0353854, 1.0300833, 0.002651033]\n",
            "Batch:125\n",
            "d_loss_real:1.0059783458709717\n",
            "d_loss_fake:0.07882940769195557\n",
            "d_loss_wrong:0.6400856971740723\n",
            "d_loss:0.6827179491519928\n",
            "g_loss:[0.85354936, 0.84605896, 0.0037451873]\n",
            "Batch:126\n",
            "d_loss_real:0.9864891171455383\n",
            "d_loss_fake:0.033034421503543854\n",
            "d_loss_wrong:0.7333046197891235\n",
            "d_loss:0.6848293244838715\n",
            "g_loss:[0.8425613, 0.83468914, 0.0039360877]\n",
            "Batch:127\n",
            "d_loss_real:1.018355131149292\n",
            "d_loss_fake:0.024342235177755356\n",
            "d_loss_wrong:0.6549599170684814\n",
            "d_loss:0.6790031045675278\n",
            "g_loss:[0.79472154, 0.78762066, 0.00355044]\n",
            "Batch:128\n",
            "d_loss_real:1.0349116325378418\n",
            "d_loss_fake:0.05053947493433952\n",
            "d_loss_wrong:0.8600634336471558\n",
            "d_loss:0.7451065480709076\n",
            "g_loss:[0.7276533, 0.71794015, 0.0048565725]\n",
            "Batch:129\n",
            "d_loss_real:1.0025503635406494\n",
            "d_loss_fake:0.0176798515021801\n",
            "d_loss_wrong:0.6845209002494812\n",
            "d_loss:0.6768253743648529\n",
            "g_loss:[0.7910697, 0.780924, 0.0050728363]\n",
            "Batch:130\n",
            "d_loss_real:0.9199261665344238\n",
            "d_loss_fake:0.09497189521789551\n",
            "d_loss_wrong:0.7191393375396729\n",
            "d_loss:0.663490891456604\n",
            "g_loss:[0.76330036, 0.75765246, 0.0028239372]\n",
            "Batch:131\n",
            "d_loss_real:0.9527227282524109\n",
            "d_loss_fake:0.03916293755173683\n",
            "d_loss_wrong:0.6742291450500488\n",
            "d_loss:0.6547093838453293\n",
            "g_loss:[0.7592117, 0.7508203, 0.0041957283]\n",
            "Batch:132\n",
            "d_loss_real:0.9671764969825745\n",
            "d_loss_fake:0.03066708892583847\n",
            "d_loss_wrong:0.6933985352516174\n",
            "d_loss:0.6646046489477158\n",
            "g_loss:[0.7924682, 0.7858768, 0.003295686]\n",
            "Batch:133\n",
            "d_loss_real:0.9099917411804199\n",
            "d_loss_fake:0.025914233177900314\n",
            "d_loss_wrong:0.6659711599349976\n",
            "d_loss:0.6279672235250473\n",
            "g_loss:[0.78705674, 0.7812371, 0.0029098052]\n",
            "Batch:134\n",
            "d_loss_real:0.9204307794570923\n",
            "d_loss_fake:0.05059546232223511\n",
            "d_loss_wrong:0.660000205039978\n",
            "d_loss:0.6378643065690994\n",
            "g_loss:[0.75016284, 0.74263155, 0.0037656387]\n",
            "Batch:135\n",
            "d_loss_real:1.0381593704223633\n",
            "d_loss_fake:0.11221760511398315\n",
            "d_loss_wrong:0.6085490584373474\n",
            "d_loss:0.6992713510990143\n",
            "g_loss:[0.80546534, 0.7995627, 0.002951324]\n",
            "Batch:136\n",
            "d_loss_real:0.9945939779281616\n",
            "d_loss_fake:0.08768996596336365\n",
            "d_loss_wrong:0.625842809677124\n",
            "d_loss:0.6756801903247833\n",
            "g_loss:[0.771721, 0.76683784, 0.0024415874]\n",
            "Batch:137\n",
            "d_loss_real:1.1710463762283325\n",
            "d_loss_fake:0.09007591009140015\n",
            "d_loss_wrong:0.6478275060653687\n",
            "d_loss:0.7699990421533585\n",
            "g_loss:[0.7851956, 0.77977175, 0.0027119361]\n",
            "Batch:138\n",
            "d_loss_real:1.0772924423217773\n",
            "d_loss_fake:0.13856105506420135\n",
            "d_loss_wrong:0.6089439988136292\n",
            "d_loss:0.7255224883556366\n",
            "g_loss:[0.82179886, 0.81579745, 0.0030006925]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnFKp4vX6GXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once in a notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19cKjFCS6Kth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6yeV-v-6M7k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7bdff2d9-1ccb-4a95-f55c-1c5f120a8985"
      },
      "source": [
        "# Create & upload a file.\n",
        "uploaded = drive.CreateFile({'title': 'stage1_dis.h5'})\n",
        "uploaded.SetContentFile('stage1_dis.h5')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1sUJewlEd15FjWkapMlORz4mXdOmusLv6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auCD-qnA6PwE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d4d25b1-7282-4cfe-af1c-7f9feeadce92"
      },
      "source": [
        "# Create & upload a file.\n",
        "uploaded = drive.CreateFile({'title': 'stage1_gen.h5'})\n",
        "uploaded.SetContentFile('stage1_gen.h5')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1ZKai3JYHkkfl36Czo2wZ5PZN1DVMdMir\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKgks-Hg6ZuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once in a notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJqorO5V6pK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkVMCk586thB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c854f743-e660-49eb-889e-ff84dd896eea"
      },
      "source": [
        "link = 'https://drive.google.com/open?id=1sUJewlEd15FjWkapMlORz4mXdOmusLv6'\n",
        "\n",
        "fluff, id = link.split('=')\n",
        "\n",
        "print (id) # Verify that you have everything after '='\n",
        "\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('stage1_dis.h5')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1sUJewlEd15FjWkapMlORz4mXdOmusLv6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQXc9dEz7U7J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5c2374f-cc22-46f6-b5ed-d735fc6cc343"
      },
      "source": [
        "link = 'https://drive.google.com/open?id=1ZKai3JYHkkfl36Czo2wZ5PZN1DVMdMir'\n",
        "\n",
        "fluff, id = link.split('=')\n",
        "\n",
        "print (id) # Verify that you have everything after '='\n",
        "\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('stage1_gen.h5')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1ZKai3JYHkkfl36Czo2wZ5PZN1DVMdMir\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3x5Nqzl73B-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "b500b88a-f86c-4a49-e3cd-05375f7a85dc"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json\tCUB_200_2011\t  logs\t    sample_data\n",
            "attributes.txt\tCUB_200_2011.tgz  results   stage1_dis.h5\n",
            "birds\t\tdrive\t\t  results2  stage1_gen.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5TmTgow75kM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "\n",
        "import PIL\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from keras import Input, Model\n",
        "from keras import backend as K\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.layers import Dense, LeakyReLU, BatchNormalization, ReLU, Reshape, UpSampling2D, Conv2D, Activation, \\\n",
        "    concatenate, Flatten, Lambda, Concatenate, ZeroPadding2D\n",
        "from keras.layers import add\n",
        "from keras.optimizers import Adam\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXQzQL757-kS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def build_ca_model():\n",
        "    \"\"\"\n",
        "    Get conditioning augmentation model.\n",
        "    Takes an embedding of shape (1024,) and returns a tensor of shape (256,)\n",
        "    \"\"\"\n",
        "    input_layer = Input(shape=(1024,))\n",
        "    x = Dense(256)(input_layer)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    model = Model(inputs=[input_layer], outputs=[x])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tgDKtr28FJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def build_embedding_compressor_model():\n",
        "    \"\"\"\n",
        "    Build embedding compressor model\n",
        "    \"\"\"\n",
        "    input_layer = Input(shape=(1024,))\n",
        "    x = Dense(128)(input_layer)\n",
        "    x = ReLU()(x)\n",
        "    model = Model(inputs=[input_layer], outputs=[x])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsEn85Qj8HyW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def generate_c(x):\n",
        "    mean = x[:, :128]\n",
        "    log_sigma = x[:, 128:]\n",
        "\n",
        "    stddev = K.exp(log_sigma)\n",
        "    epsilon = K.random_normal(shape=K.constant((mean.shape[1],), dtype='int32'))\n",
        "    c = stddev * epsilon + mean\n",
        "\n",
        "    return c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYfhthRH9Pce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_stage1_generator():\n",
        "    \"\"\"\n",
        "    Builds a generator model used in Stage-I\n",
        "    \"\"\"\n",
        "    input_layer = Input(shape=(1024,))\n",
        "    x = Dense(256)(input_layer)\n",
        "    mean_logsigma = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    c = Lambda(generate_c)(mean_logsigma)\n",
        "\n",
        "    input_layer2 = Input(shape=(100,))\n",
        "\n",
        "    gen_input = Concatenate(axis=1)([c, input_layer2])\n",
        "\n",
        "    x = Dense(128 * 8 * 4 * 4, use_bias=False)(gen_input)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = Reshape((4, 4, 128 * 8), input_shape=(128 * 8 * 4 * 4,))(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(512, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(256, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(128, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(64, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = Conv2D(3, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = Activation(activation='tanh')(x)\n",
        "\n",
        "    stage1_gen = Model(inputs=[input_layer, input_layer2], outputs=[x, mean_logsigma])\n",
        "    return stage1_gen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pj7Bx2qk9QwZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def residual_block(input):\n",
        "    \"\"\"\n",
        "    Residual block in the generator network\n",
        "    \"\"\"\n",
        "    x = Conv2D(128 * 4, kernel_size=(3, 3), padding='same', strides=1)(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = Conv2D(128 * 4, kernel_size=(3, 3), strides=1, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = add([x, input])\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWFMh30g9To9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def joint_block(inputs):\n",
        "    c = inputs[0]\n",
        "    x = inputs[1]\n",
        "\n",
        "    c = K.expand_dims(c, axis=1)\n",
        "    c = K.expand_dims(c, axis=1)\n",
        "    c = K.tile(c, [1, 16, 16, 1])\n",
        "    return K.concatenate([c, x], axis=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAZE1bK29Woi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_stage2_generator():\n",
        "    \"\"\"\n",
        "    Create Stage-II generator containing the CA Augmentation Network,\n",
        "    the image encoder and the generator network\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. CA Augmentation Network\n",
        "    input_layer = Input(shape=(1024,))\n",
        "    input_lr_images = Input(shape=(64, 64, 3))\n",
        "\n",
        "    ca = Dense(256)(input_layer)\n",
        "    mean_logsigma = LeakyReLU(alpha=0.2)(ca)\n",
        "    c = Lambda(generate_c)(mean_logsigma)\n",
        "\n",
        "    # 2. Image Encoder\n",
        "    x = ZeroPadding2D(padding=(1, 1))(input_lr_images)\n",
        "    x = Conv2D(128, kernel_size=(3, 3), strides=1, use_bias=False)(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
        "    x = Conv2D(256, kernel_size=(4, 4), strides=2, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
        "    x = Conv2D(512, kernel_size=(4, 4), strides=2, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    # 3. Joint\n",
        "    c_code = Lambda(joint_block)([c, x])\n",
        "\n",
        "    x = ZeroPadding2D(padding=(1, 1))(c_code)\n",
        "    x = Conv2D(512, kernel_size=(3, 3), strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    # 4. Residual blocks\n",
        "    x = residual_block(x)\n",
        "    x = residual_block(x)\n",
        "    x = residual_block(x)\n",
        "    x = residual_block(x)\n",
        "\n",
        "    # 5. Upsampling blocks\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(512, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(256, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(128, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(64, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = Conv2D(3, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = Activation('tanh')(x)\n",
        "\n",
        "    model = Model(inputs=[input_layer, input_lr_images], outputs=[x, mean_logsigma])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWJkrTJ49ZWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_stage2_discriminator():\n",
        "    \"\"\"\n",
        "    Create Stage-II discriminator network\n",
        "    \"\"\"\n",
        "    input_layer = Input(shape=(256, 256, 3))\n",
        "\n",
        "    x = Conv2D(64, (4, 4), padding='same', strides=2, input_shape=(256, 256, 3), use_bias=False)(input_layer)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(128, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(256, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(512, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(1024, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(2048, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(1024, (1, 1), padding='same', strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(512, (1, 1), padding='same', strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x2 = Conv2D(128, (1, 1), padding='same', strides=1, use_bias=False)(x)\n",
        "    x2 = BatchNormalization()(x2)\n",
        "    x2 = LeakyReLU(alpha=0.2)(x2)\n",
        "\n",
        "    x2 = Conv2D(128, (3, 3), padding='same', strides=1, use_bias=False)(x2)\n",
        "    x2 = BatchNormalization()(x2)\n",
        "    x2 = LeakyReLU(alpha=0.2)(x2)\n",
        "\n",
        "    x2 = Conv2D(512, (3, 3), padding='same', strides=1, use_bias=False)(x2)\n",
        "    x2 = BatchNormalization()(x2)\n",
        "\n",
        "    added_x = add([x, x2])\n",
        "    added_x = LeakyReLU(alpha=0.2)(added_x)\n",
        "\n",
        "    input_layer2 = Input(shape=(4, 4, 128))\n",
        "\n",
        "    merged_input = concatenate([added_x, input_layer2])\n",
        "\n",
        "    x3 = Conv2D(64 * 8, kernel_size=1, padding=\"same\", strides=1)(merged_input)\n",
        "    x3 = BatchNormalization()(x3)\n",
        "    x3 = LeakyReLU(alpha=0.2)(x3)\n",
        "    x3 = Flatten()(x3)\n",
        "    x3 = Dense(1)(x3)\n",
        "    x3 = Activation('sigmoid')(x3)\n",
        "\n",
        "    stage2_dis = Model(inputs=[input_layer, input_layer2], outputs=[x3])\n",
        "    return stage2_dis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9q5x7Zp9cRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_adversarial_model(gen_model2, dis_model, gen_model1):\n",
        "    \"\"\"\n",
        "    Create adversarial model\n",
        "    \"\"\"\n",
        "    embeddings_input_layer = Input(shape=(1024, ))\n",
        "    noise_input_layer = Input(shape=(100, ))\n",
        "    compressed_embedding_input_layer = Input(shape=(4, 4, 128))\n",
        "\n",
        "    gen_model1.trainable = False\n",
        "    dis_model.trainable = False\n",
        "\n",
        "    lr_images, mean_logsigma1 = gen_model1([embeddings_input_layer, noise_input_layer])\n",
        "    hr_images, mean_logsigma2 = gen_model2([embeddings_input_layer, lr_images])\n",
        "    valid = dis_model([hr_images, compressed_embedding_input_layer])\n",
        "\n",
        "    model = Model(inputs=[embeddings_input_layer, noise_input_layer, compressed_embedding_input_layer], outputs=[valid, mean_logsigma2])\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "037BHVXA9fJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Dataset loading related methods\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def load_class_ids(class_info_file_path):\n",
        "    \"\"\"\n",
        "    Load class ids from class_info.pickle file\n",
        "    \"\"\"\n",
        "    with open(class_info_file_path, 'rb') as f:\n",
        "        class_ids = pickle.load(f, encoding='latin1')\n",
        "        return class_ids\n",
        "\n",
        "\n",
        "def load_embeddings(embeddings_file_path):\n",
        "    \"\"\"\n",
        "    Function to load embeddings\n",
        "    \"\"\"\n",
        "    with open(embeddings_file_path, 'rb') as f:\n",
        "        embeddings = pickle.load(f, encoding='latin1')\n",
        "        embeddings = np.array(embeddings)\n",
        "        print('embeddings: ', embeddings.shape)\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "def load_filenames(filenames_file_path):\n",
        "    \"\"\"\n",
        "    Load filenames.pickle file and return a list of all file names\n",
        "    \"\"\"\n",
        "    with open(filenames_file_path, 'rb') as f:\n",
        "        filenames = pickle.load(f, encoding='latin1')\n",
        "    return filenames\n",
        "\n",
        "\n",
        "def load_bounding_boxes(dataset_dir):\n",
        "    \"\"\"\n",
        "    Load bounding boxes and return a dictionary of file names and corresponding bounding boxes\n",
        "    \"\"\"\n",
        "    # Paths\n",
        "    bounding_boxes_path = os.path.join(dataset_dir, 'bounding_boxes.txt')\n",
        "    file_paths_path = os.path.join(dataset_dir, 'images.txt')\n",
        "\n",
        "    # Read bounding_boxes.txt and images.txt file\n",
        "    df_bounding_boxes = pd.read_csv(bounding_boxes_path,\n",
        "                                    delim_whitespace=True, header=None).astype(int)\n",
        "    df_file_names = pd.read_csv(file_paths_path, delim_whitespace=True, header=None)\n",
        "\n",
        "    # Create a list of file names\n",
        "    file_names = df_file_names[1].tolist()\n",
        "\n",
        "    # Create a dictionary of file_names and bounding boxes\n",
        "    filename_boundingbox_dict = {img_file[:-4]: [] for img_file in file_names[:2]}\n",
        "\n",
        "    # Assign a bounding box to the corresponding image\n",
        "    for i in range(0, len(file_names)):\n",
        "        # Get the bounding box\n",
        "        bounding_box = df_bounding_boxes.iloc[i][1:].tolist()\n",
        "        key = file_names[i][:-4]\n",
        "        filename_boundingbox_dict[key] = bounding_box\n",
        "\n",
        "    return filename_boundingbox_dict\n",
        "\n",
        "\n",
        "def get_img(img_path, bbox, image_size):\n",
        "    \"\"\"\n",
        "    Load and resize images\n",
        "    \"\"\"\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    width, height = img.size\n",
        "    if bbox is not None:\n",
        "        R = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
        "        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
        "        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
        "        y1 = np.maximum(0, center_y - R)\n",
        "        y2 = np.minimum(height, center_y + R)\n",
        "        x1 = np.maximum(0, center_x - R)\n",
        "        x2 = np.minimum(width, center_x + R)\n",
        "        img = img.crop([x1, y1, x2, y2])\n",
        "    img = img.resize(image_size, PIL.Image.BILINEAR)\n",
        "    return img\n",
        "\n",
        "\n",
        "def load_dataset(filenames_file_path, class_info_file_path, cub_dataset_dir, embeddings_file_path, image_size):\n",
        "    filenames = load_filenames(filenames_file_path)\n",
        "    class_ids = load_class_ids(class_info_file_path)\n",
        "    bounding_boxes = load_bounding_boxes(cub_dataset_dir)\n",
        "    all_embeddings = load_embeddings(embeddings_file_path)\n",
        "\n",
        "    X, y, embeddings = [], [], []\n",
        "\n",
        "    print(\"All embeddings shape:\", all_embeddings.shape)\n",
        "\n",
        "    for index, filename in enumerate(filenames):\n",
        "        bounding_box = bounding_boxes[filename]\n",
        "\n",
        "        try:\n",
        "            # Load images\n",
        "            img_name = '{}/images/{}.jpg'.format(cub_dataset_dir, filename)\n",
        "            img = get_img(img_name, bounding_box, image_size)\n",
        "\n",
        "            all_embeddings1 = all_embeddings[index, :, :]\n",
        "\n",
        "            embedding_ix = random.randint(0, all_embeddings1.shape[0] - 1)\n",
        "            embedding = all_embeddings1[embedding_ix, :]\n",
        "\n",
        "            X.append(np.array(img))\n",
        "            y.append(class_ids[index])\n",
        "            embeddings.append(embedding)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    embeddings = np.array(embeddings)\n",
        "\n",
        "    return X, y, embeddings\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9EmHVa_9hwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Loss functions\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def KL_loss(y_true, y_pred):\n",
        "    mean = y_pred[:, :128]\n",
        "    logsigma = y_pred[:, :128]\n",
        "    loss = -logsigma + .5 * (-1 + K.exp(2. * logsigma) + K.square(mean))\n",
        "    loss = K.mean(loss)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def custom_generator_loss(y_true, y_pred):\n",
        "    # Calculate binary cross entropy loss\n",
        "    return K.binary_crossentropy(y_true, y_pred)\n",
        "\n",
        "\n",
        "def write_log(callback, name, loss, batch_no):\n",
        "    \"\"\"\n",
        "    Write training summary to TensorBoard\n",
        "    \"\"\"\n",
        "    summary = tf.Summary()\n",
        "    summary_value = summary.value.add()\n",
        "    summary_value.simple_value = loss\n",
        "    summary_value.tag = name\n",
        "    callback.writer.add_summary(summary, batch_no)\n",
        "    callback.writer.flush()\n",
        "\n",
        "\n",
        "def save_rgb_img(img, path):\n",
        "    \"\"\"\n",
        "    Save an rgb image\n",
        "    \"\"\"\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.imshow(img)\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(\"Image\")\n",
        "\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rTwy71O9lA8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 14401
        },
        "outputId": "69a686d1-7a1b-4cda-f9bc-a59ac2583ce4"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    data_dir = \"/content/birds/\"\n",
        "    train_dir = data_dir + \"/train\"\n",
        "    test_dir = data_dir + \"/test\"\n",
        "    hr_image_size = (256, 256)\n",
        "    lr_image_size = (64, 64)\n",
        "    batch_size = 32\n",
        "    z_dim = 100\n",
        "    stage1_generator_lr = 0.0002\n",
        "    stage1_discriminator_lr = 0.0002\n",
        "    stage1_lr_decay_step = 600\n",
        "    epochs = 1\n",
        "    condition_dim = 128\n",
        "\n",
        "    embeddings_file_path_train = train_dir + \"/char-CNN-RNN-embeddings.pickle\"\n",
        "    embeddings_file_path_test = test_dir + \"/char-CNN-RNN-embeddings.pickle\"\n",
        "\n",
        "    filenames_file_path_train = train_dir + \"/filenames.pickle\"\n",
        "    filenames_file_path_test = test_dir + \"/filenames.pickle\"\n",
        "\n",
        "    class_info_file_path_train = train_dir + \"/class_info.pickle\"\n",
        "    class_info_file_path_test = test_dir + \"/class_info.pickle\"\n",
        "\n",
        "    cub_dataset_dir = \"/content/CUB_200_2011\"\n",
        "\n",
        "    # Define optimizers\n",
        "    dis_optimizer = Adam(lr=stage1_discriminator_lr, beta_1=0.5, beta_2=0.999)\n",
        "    gen_optimizer = Adam(lr=stage1_generator_lr, beta_1=0.5, beta_2=0.999)\n",
        "\n",
        "    \"\"\"\n",
        "    Load datasets\n",
        "    \"\"\"\n",
        "    X_hr_train, y_hr_train, embeddings_train = load_dataset(filenames_file_path=filenames_file_path_train,\n",
        "                                                            class_info_file_path=class_info_file_path_train,\n",
        "                                                            cub_dataset_dir=cub_dataset_dir,\n",
        "                                                            embeddings_file_path=embeddings_file_path_train,\n",
        "                                                            image_size=(256, 256))\n",
        "\n",
        "    X_hr_test, y_hr_test, embeddings_test = load_dataset(filenames_file_path=filenames_file_path_test,\n",
        "                                                         class_info_file_path=class_info_file_path_test,\n",
        "                                                         cub_dataset_dir=cub_dataset_dir,\n",
        "                                                         embeddings_file_path=embeddings_file_path_test,\n",
        "                                                         image_size=(256, 256))\n",
        "\n",
        "    X_lr_train, y_lr_train, _ = load_dataset(filenames_file_path=filenames_file_path_train,\n",
        "                                             class_info_file_path=class_info_file_path_train,\n",
        "                                             cub_dataset_dir=cub_dataset_dir,\n",
        "                                             embeddings_file_path=embeddings_file_path_train,\n",
        "                                             image_size=(64, 64))\n",
        "\n",
        "    X_lr_test, y_lr_test, _ = load_dataset(filenames_file_path=filenames_file_path_test,\n",
        "                                           class_info_file_path=class_info_file_path_test,\n",
        "                                           cub_dataset_dir=cub_dataset_dir,\n",
        "                                           embeddings_file_path=embeddings_file_path_test,\n",
        "                                           image_size=(64, 64))\n",
        "\n",
        "    \"\"\"\n",
        "    Build and compile models\n",
        "    \"\"\"\n",
        "    stage2_dis = build_stage2_discriminator()\n",
        "    stage2_dis.compile(loss='binary_crossentropy', optimizer=dis_optimizer)\n",
        "\n",
        "    stage1_gen = build_stage1_generator()\n",
        "    stage1_gen.compile(loss=\"binary_crossentropy\", optimizer=gen_optimizer)\n",
        "\n",
        "    stage1_gen.load_weights(\"stage1_gen.h5\")\n",
        "\n",
        "    stage2_gen = build_stage2_generator()\n",
        "    stage2_gen.compile(loss=\"binary_crossentropy\", optimizer=gen_optimizer)\n",
        "\n",
        "    embedding_compressor_model = build_embedding_compressor_model()\n",
        "    embedding_compressor_model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "    adversarial_model = build_adversarial_model(stage2_gen, stage2_dis, stage1_gen)\n",
        "    adversarial_model.compile(loss=['binary_crossentropy', KL_loss], loss_weights=[1.0, 2.0],\n",
        "                              optimizer=gen_optimizer, metrics=None)\n",
        "\n",
        "    tensorboard = TensorBoard(log_dir=\"logs/\".format(time.time()))\n",
        "    tensorboard.set_model(stage2_gen)\n",
        "    tensorboard.set_model(stage2_dis)\n",
        "\n",
        "    # Generate an array containing real and fake values\n",
        "    # Apply label smoothing\n",
        "    real_labels = np.ones((batch_size, 1), dtype=float) * 0.9\n",
        "    fake_labels = np.zeros((batch_size, 1), dtype=float) * 0.1\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(\"========================================\")\n",
        "        print(\"Epoch is:\", epoch)\n",
        "\n",
        "        gen_losses = []\n",
        "        dis_losses = []\n",
        "\n",
        "        # Load data and train model\n",
        "        number_of_batches = int(X_hr_train.shape[0] / batch_size)\n",
        "        print(\"Number of batches:{}\".format(number_of_batches))\n",
        "        for index in range(number_of_batches):\n",
        "            print(\"Batch:{}\".format(index+1))\n",
        "\n",
        "            # Create a noise vector\n",
        "            z_noise = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
        "            X_hr_train_batch = X_hr_train[index * batch_size:(index + 1) * batch_size]\n",
        "            embedding_batch = embeddings_train[index * batch_size:(index + 1) * batch_size]\n",
        "            X_hr_train_batch = (X_hr_train_batch - 127.5) / 127.5\n",
        "\n",
        "            # Generate fake images\n",
        "            lr_fake_images, _ = stage1_gen.predict([embedding_batch, z_noise], verbose=3)\n",
        "            hr_fake_images, _ = stage2_gen.predict([embedding_batch, lr_fake_images], verbose=3)\n",
        "\n",
        "            \"\"\"\n",
        "            4. Generate compressed embeddings\n",
        "            \"\"\"\n",
        "            compressed_embedding = embedding_compressor_model.predict_on_batch(embedding_batch)\n",
        "            compressed_embedding = np.reshape(compressed_embedding, (-1, 1, 1, condition_dim))\n",
        "            compressed_embedding = np.tile(compressed_embedding, (1, 4, 4, 1))\n",
        "\n",
        "            \"\"\"\n",
        "            5. Train the discriminator model\n",
        "            \"\"\"\n",
        "            dis_loss_real = stage2_dis.train_on_batch([X_hr_train_batch, compressed_embedding],\n",
        "                                                      np.reshape(real_labels, (batch_size, 1)))\n",
        "            dis_loss_fake = stage2_dis.train_on_batch([hr_fake_images, compressed_embedding],\n",
        "                                                      np.reshape(fake_labels, (batch_size, 1)))\n",
        "            dis_loss_wrong = stage2_dis.train_on_batch([X_hr_train_batch[:(batch_size - 1)], compressed_embedding[1:]],\n",
        "                                                       np.reshape(fake_labels[1:], (batch_size-1, 1)))\n",
        "            d_loss = 0.5 * np.add(dis_loss_real, 0.5 * np.add(dis_loss_wrong,  dis_loss_fake))\n",
        "            print(\"d_loss:{}\".format(d_loss))\n",
        "\n",
        "            \"\"\"\n",
        "            Train the adversarial model\n",
        "            \"\"\"\n",
        "            g_loss = adversarial_model.train_on_batch([embedding_batch, z_noise, compressed_embedding],\n",
        "                                                                [K.ones((batch_size, 1)) * 0.9, K.ones((batch_size, 256)) * 0.9])\n",
        "\n",
        "            print(\"g_loss:{}\".format(g_loss))\n",
        "\n",
        "            dis_losses.append(d_loss)\n",
        "            gen_losses.append(g_loss)\n",
        "\n",
        "        \"\"\"\n",
        "        Save losses to Tensorboard after each epoch\n",
        "        \"\"\"\n",
        "        write_log(tensorboard, 'discriminator_loss', np.mean(dis_losses), epoch)\n",
        "        write_log(tensorboard, 'generator_loss', np.mean(gen_losses[0]), epoch)\n",
        "\n",
        "        # Generate and save images after every 2nd epoch\n",
        "        if epoch % 2 == 0:\n",
        "            # z_noise2 = np.random.uniform(-1, 1, size=(batch_size, z_dim))\n",
        "            z_noise2 = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
        "            embedding_batch = embeddings_test[0:batch_size]\n",
        "\n",
        "            lr_fake_images, _ = stage1_gen.predict([embedding_batch, z_noise2], verbose=3)\n",
        "            hr_fake_images, _ = stage2_gen.predict([embedding_batch, lr_fake_images], verbose=3)\n",
        "\n",
        "            # Save images\n",
        "            for i, img in enumerate(hr_fake_images[:10]):\n",
        "                save_rgb_img(img, \"results2/gen_{}_{}.png\".format(epoch, i))\n",
        "\n",
        "    # Saving the models\n",
        "    stage2_gen.save_weights(\"stage2_gen.h5\")\n",
        "    stage2_dis.save_weights(\"stage2_dis.h5\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embeddings:  (8855, 10, 1024)\n",
            "All embeddings shape: (8855, 10, 1024)\n",
            "embeddings:  (2933, 10, 1024)\n",
            "All embeddings shape: (2933, 10, 1024)\n",
            "embeddings:  (8855, 10, 1024)\n",
            "All embeddings shape: (8855, 10, 1024)\n",
            "embeddings:  (2933, 10, 1024)\n",
            "All embeddings shape: (2933, 10, 1024)\n",
            "========================================\n",
            "Epoch is: 0\n",
            "Number of batches:276\n",
            "Batch:1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "d_loss:4.6952764093875885\n",
            "g_loss:[1.2072113, 1.1752551, 0.015978077]\n",
            "Batch:2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "d_loss:1.8994561433792114\n",
            "g_loss:[1.0797867, 1.0531601, 0.0133133]\n",
            "Batch:3\n",
            "d_loss:2.214909553527832\n",
            "g_loss:[1.5063928, 1.4685576, 0.018917594]\n",
            "Batch:4\n",
            "d_loss:1.8273532390594482\n",
            "g_loss:[2.7607906, 2.717936, 0.021427315]\n",
            "Batch:5\n",
            "d_loss:2.0141799449920654\n",
            "g_loss:[1.0044757, 0.9626559, 0.020909918]\n",
            "Batch:6\n",
            "d_loss:1.3769708573818207\n",
            "g_loss:[1.1171458, 1.0898525, 0.013646672]\n",
            "Batch:7\n",
            "d_loss:1.2214335799217224\n",
            "g_loss:[0.76904106, 0.7382412, 0.015399941]\n",
            "Batch:8\n",
            "d_loss:1.5624951124191284\n",
            "g_loss:[1.062635, 1.0136571, 0.024488907]\n",
            "Batch:9\n",
            "d_loss:0.8398782908916473\n",
            "g_loss:[1.8586442, 1.8290017, 0.014821296]\n",
            "Batch:10\n",
            "d_loss:1.4035927504301071\n",
            "g_loss:[1.2239299, 1.1986154, 0.0126572335]\n",
            "Batch:11\n",
            "d_loss:1.2003048658370972\n",
            "g_loss:[1.1244291, 1.0987549, 0.012837099]\n",
            "Batch:12\n",
            "d_loss:1.062582939863205\n",
            "g_loss:[1.5016068, 1.4712589, 0.015173983]\n",
            "Batch:13\n",
            "d_loss:1.0509536415338516\n",
            "g_loss:[1.592558, 1.5686116, 0.011973221]\n",
            "Batch:14\n",
            "d_loss:1.0798986852169037\n",
            "g_loss:[1.303658, 1.2828915, 0.010383271]\n",
            "Batch:15\n",
            "d_loss:1.172542691230774\n",
            "g_loss:[1.4701245, 1.4258313, 0.022146555]\n",
            "Batch:16\n",
            "d_loss:0.884967029094696\n",
            "g_loss:[1.2669884, 1.2289811, 0.019003613]\n",
            "Batch:17\n",
            "d_loss:1.0378873944282532\n",
            "g_loss:[1.2109914, 1.167437, 0.021777231]\n",
            "Batch:18\n",
            "d_loss:0.8740012049674988\n",
            "g_loss:[1.1301042, 1.0966837, 0.016710194]\n",
            "Batch:19\n",
            "d_loss:0.825768917798996\n",
            "g_loss:[1.0771234, 1.0399135, 0.018604938]\n",
            "Batch:20\n",
            "d_loss:1.018771082162857\n",
            "g_loss:[0.92221105, 0.89112717, 0.015541935]\n",
            "Batch:21\n",
            "d_loss:0.955700695514679\n",
            "g_loss:[1.2096322, 1.178256, 0.015688075]\n",
            "Batch:22\n",
            "d_loss:0.90911865234375\n",
            "g_loss:[0.91411674, 0.887606, 0.013255363]\n",
            "Batch:23\n",
            "d_loss:0.7315762490034103\n",
            "g_loss:[0.8959674, 0.8752961, 0.010335643]\n",
            "Batch:24\n",
            "d_loss:0.9081089794635773\n",
            "g_loss:[0.8013013, 0.7813562, 0.009972538]\n",
            "Batch:25\n",
            "d_loss:0.7240496277809143\n",
            "g_loss:[0.7340477, 0.71527445, 0.009386634]\n",
            "Batch:26\n",
            "d_loss:0.7245977520942688\n",
            "g_loss:[0.70887417, 0.6963965, 0.006238812]\n",
            "Batch:27\n",
            "d_loss:0.7953296005725861\n",
            "g_loss:[0.7890034, 0.7734256, 0.0077888896]\n",
            "Batch:28\n",
            "d_loss:0.7379391938447952\n",
            "g_loss:[0.7344856, 0.7137882, 0.010348693]\n",
            "Batch:29\n",
            "d_loss:0.7384796142578125\n",
            "g_loss:[0.7902925, 0.7680061, 0.011143195]\n",
            "Batch:30\n",
            "d_loss:0.7058943957090378\n",
            "g_loss:[1.0969695, 1.0769265, 0.010021521]\n",
            "Batch:31\n",
            "d_loss:0.9518986791372299\n",
            "g_loss:[0.84895194, 0.8289302, 0.01001087]\n",
            "Batch:32\n",
            "d_loss:0.7955780774354935\n",
            "g_loss:[0.7314927, 0.71687865, 0.00730701]\n",
            "Batch:33\n",
            "d_loss:0.8414731621742249\n",
            "g_loss:[1.0199087, 1.0100281, 0.0049402528]\n",
            "Batch:34\n",
            "d_loss:0.7563904970884323\n",
            "g_loss:[0.9415954, 0.93264806, 0.0044736573]\n",
            "Batch:35\n",
            "d_loss:0.8326520919799805\n",
            "g_loss:[1.0242789, 1.015279, 0.004499889]\n",
            "Batch:36\n",
            "d_loss:0.830899715423584\n",
            "g_loss:[0.9741227, 0.96213686, 0.005992919]\n",
            "Batch:37\n",
            "d_loss:0.7938744127750397\n",
            "g_loss:[1.1350125, 1.1242781, 0.0053671924]\n",
            "Batch:38\n",
            "d_loss:0.9395706355571747\n",
            "g_loss:[1.1484145, 1.1408325, 0.0037909849]\n",
            "Batch:39\n",
            "d_loss:0.7857732027769089\n",
            "g_loss:[1.0183041, 1.0103637, 0.003970205]\n",
            "Batch:40\n",
            "d_loss:0.940881997346878\n",
            "g_loss:[0.9861117, 0.975156, 0.0054778527]\n",
            "Batch:41\n",
            "d_loss:0.8760805130004883\n",
            "g_loss:[0.91868734, 0.9056003, 0.00654351]\n",
            "Batch:42\n",
            "d_loss:0.8734268248081207\n",
            "g_loss:[0.82259935, 0.8105997, 0.0059998306]\n",
            "Batch:43\n",
            "d_loss:0.8082955479621887\n",
            "g_loss:[0.81614286, 0.8046844, 0.0057292297]\n",
            "Batch:44\n",
            "d_loss:0.8762640953063965\n",
            "g_loss:[0.99846953, 0.9864342, 0.006017654]\n",
            "Batch:45\n",
            "d_loss:0.7614910304546356\n",
            "g_loss:[0.7483951, 0.7403338, 0.004030645]\n",
            "Batch:46\n",
            "d_loss:0.7327061891555786\n",
            "g_loss:[0.73727864, 0.7258762, 0.005701228]\n",
            "Batch:47\n",
            "d_loss:0.7194250971078873\n",
            "g_loss:[0.7565379, 0.7448983, 0.0058198064]\n",
            "Batch:48\n",
            "d_loss:0.7184125036001205\n",
            "g_loss:[0.7931223, 0.7820468, 0.0055377525]\n",
            "Batch:49\n",
            "d_loss:0.8269731998443604\n",
            "g_loss:[0.6373181, 0.6233934, 0.0069623254]\n",
            "Batch:50\n",
            "d_loss:0.7142252326011658\n",
            "g_loss:[0.6033188, 0.59187156, 0.0057236333]\n",
            "Batch:51\n",
            "d_loss:0.7451233714818954\n",
            "g_loss:[0.85838974, 0.8495118, 0.004438962]\n",
            "Batch:52\n",
            "d_loss:0.6972236335277557\n",
            "g_loss:[0.69873023, 0.68924665, 0.004741774]\n",
            "Batch:53\n",
            "d_loss:0.7246523052453995\n",
            "g_loss:[0.7081784, 0.6971145, 0.0055319346]\n",
            "Batch:54\n",
            "d_loss:0.7158655822277069\n",
            "g_loss:[0.8713371, 0.85978913, 0.0057739913]\n",
            "Batch:55\n",
            "d_loss:0.6758104562759399\n",
            "g_loss:[1.0020678, 0.986562, 0.0077528795]\n",
            "Batch:56\n",
            "d_loss:0.8543844223022461\n",
            "g_loss:[0.89564484, 0.87711626, 0.009264292]\n",
            "Batch:57\n",
            "d_loss:0.7450134009122849\n",
            "g_loss:[0.8474499, 0.83297855, 0.007235679]\n",
            "Batch:58\n",
            "d_loss:0.6621511876583099\n",
            "g_loss:[0.8165801, 0.805869, 0.0053555756]\n",
            "Batch:59\n",
            "d_loss:0.6947824507951736\n",
            "g_loss:[0.8554256, 0.8476184, 0.0039036083]\n",
            "Batch:60\n",
            "d_loss:0.6352936625480652\n",
            "g_loss:[0.77561563, 0.7685633, 0.0035261833]\n",
            "Batch:61\n",
            "d_loss:0.628066211938858\n",
            "g_loss:[0.7976898, 0.7918741, 0.0029078338]\n",
            "Batch:62\n",
            "d_loss:0.6512946337461472\n",
            "g_loss:[0.8255758, 0.81655467, 0.004510589]\n",
            "Batch:63\n",
            "d_loss:0.7040555328130722\n",
            "g_loss:[0.92227346, 0.912267, 0.0050032153]\n",
            "Batch:64\n",
            "d_loss:0.7678756862878799\n",
            "g_loss:[0.91342396, 0.9043158, 0.0045540505]\n",
            "Batch:65\n",
            "d_loss:0.8037090301513672\n",
            "g_loss:[0.96655405, 0.95760876, 0.004472645]\n",
            "Batch:66\n",
            "d_loss:0.7548860609531403\n",
            "g_loss:[0.9000598, 0.8919808, 0.0040394855]\n",
            "Batch:67\n",
            "d_loss:0.8018266260623932\n",
            "g_loss:[0.8822335, 0.8763672, 0.002933147]\n",
            "Batch:68\n",
            "d_loss:0.7515078634023666\n",
            "g_loss:[0.7986274, 0.79152155, 0.0035529006]\n",
            "Batch:69\n",
            "d_loss:0.7034619748592377\n",
            "g_loss:[0.7615651, 0.753726, 0.003919531]\n",
            "Batch:70\n",
            "d_loss:0.6851865947246552\n",
            "g_loss:[0.8627052, 0.8553268, 0.0036892125]\n",
            "Batch:71\n",
            "d_loss:0.6671072393655777\n",
            "g_loss:[1.1856484, 1.1798308, 0.0029088075]\n",
            "Batch:72\n",
            "d_loss:1.1598195731639862\n",
            "g_loss:[0.9196722, 0.91026187, 0.004705147]\n",
            "Batch:73\n",
            "d_loss:0.6617434471845627\n",
            "g_loss:[0.77892613, 0.7703251, 0.004300508]\n",
            "Batch:74\n",
            "d_loss:0.6857210546731949\n",
            "g_loss:[0.8116442, 0.8035878, 0.004028198]\n",
            "Batch:75\n",
            "d_loss:0.7008017152547836\n",
            "g_loss:[0.82932884, 0.82124984, 0.004039486]\n",
            "Batch:76\n",
            "d_loss:0.729162335395813\n",
            "g_loss:[0.75450927, 0.7470597, 0.00372477]\n",
            "Batch:77\n",
            "d_loss:0.6204901784658432\n",
            "g_loss:[0.816564, 0.8075818, 0.0044911196]\n",
            "Batch:78\n",
            "d_loss:0.6686743497848511\n",
            "g_loss:[0.75411034, 0.7466922, 0.0037090855]\n",
            "Batch:79\n",
            "d_loss:0.6227325350046158\n",
            "g_loss:[0.7009794, 0.691285, 0.0048471848]\n",
            "Batch:80\n",
            "d_loss:0.670995369553566\n",
            "g_loss:[0.7154174, 0.705513, 0.004952192]\n",
            "Batch:81\n",
            "d_loss:0.6196433752775192\n",
            "g_loss:[0.6731661, 0.66588926, 0.0036384112]\n",
            "Batch:82\n",
            "d_loss:0.7078335285186768\n",
            "g_loss:[0.65741885, 0.6525371, 0.0024408759]\n",
            "Batch:83\n",
            "d_loss:0.6839968264102936\n",
            "g_loss:[0.59822387, 0.59303033, 0.002596767]\n",
            "Batch:84\n",
            "d_loss:0.6479687243700027\n",
            "g_loss:[0.6464858, 0.63989604, 0.003294889]\n",
            "Batch:85\n",
            "d_loss:0.5962596833705902\n",
            "g_loss:[0.59802157, 0.5896375, 0.0041920384]\n",
            "Batch:86\n",
            "d_loss:0.6511778682470322\n",
            "g_loss:[0.65369606, 0.6456503, 0.0040228725]\n",
            "Batch:87\n",
            "d_loss:0.644296333193779\n",
            "g_loss:[0.6634487, 0.6547242, 0.0043622414]\n",
            "Batch:88\n",
            "d_loss:0.628322422504425\n",
            "g_loss:[0.7189959, 0.71083593, 0.0040799845]\n",
            "Batch:89\n",
            "d_loss:0.5742849260568619\n",
            "g_loss:[0.670297, 0.6633841, 0.0034564761]\n",
            "Batch:90\n",
            "d_loss:0.7648607194423676\n",
            "g_loss:[0.7198044, 0.7123855, 0.0037094713]\n",
            "Batch:91\n",
            "d_loss:0.7083660215139389\n",
            "g_loss:[0.6754769, 0.668073, 0.0037019467]\n",
            "Batch:92\n",
            "d_loss:0.6417324393987656\n",
            "g_loss:[0.6781326, 0.6738254, 0.0021535978]\n",
            "Batch:93\n",
            "d_loss:0.6463449895381927\n",
            "g_loss:[0.66888833, 0.66393507, 0.0024766205]\n",
            "Batch:94\n",
            "d_loss:0.696282371878624\n",
            "g_loss:[0.6876822, 0.6790419, 0.0043201335]\n",
            "Batch:95\n",
            "d_loss:0.7431115359067917\n",
            "g_loss:[0.64811605, 0.63912547, 0.0044952943]\n",
            "Batch:96\n",
            "d_loss:0.6744268536567688\n",
            "g_loss:[0.665032, 0.6599711, 0.0025304498]\n",
            "Batch:97\n",
            "d_loss:0.6271151751279831\n",
            "g_loss:[1.1698345, 1.163555, 0.003139727]\n",
            "Batch:98\n",
            "d_loss:0.9067117273807526\n",
            "g_loss:[1.036946, 1.0296196, 0.0036632472]\n",
            "Batch:99\n",
            "d_loss:0.6958387792110443\n",
            "g_loss:[1.2266932, 1.2206655, 0.0030138397]\n",
            "Batch:100\n",
            "d_loss:0.7826285511255264\n",
            "g_loss:[0.9242681, 0.91943514, 0.0024165004]\n",
            "Batch:101\n",
            "d_loss:0.6550717055797577\n",
            "g_loss:[0.8383038, 0.83403015, 0.0021368316]\n",
            "Batch:102\n",
            "d_loss:0.6742817014455795\n",
            "g_loss:[0.82173383, 0.81633383, 0.0026999868]\n",
            "Batch:103\n",
            "d_loss:0.6423337757587433\n",
            "g_loss:[0.83166665, 0.8282858, 0.0016904195]\n",
            "Batch:104\n",
            "d_loss:0.7036600112915039\n",
            "g_loss:[0.8654441, 0.862563, 0.0014405516]\n",
            "Batch:105\n",
            "d_loss:0.6353629231452942\n",
            "g_loss:[0.8792771, 0.87503296, 0.002122066]\n",
            "Batch:106\n",
            "d_loss:0.743086040019989\n",
            "g_loss:[0.80530035, 0.8002552, 0.0025225896]\n",
            "Batch:107\n",
            "d_loss:0.611517459154129\n",
            "g_loss:[0.76377666, 0.7585266, 0.0026250079]\n",
            "Batch:108\n",
            "d_loss:0.609085738658905\n",
            "g_loss:[0.7570435, 0.752807, 0.0021182299]\n",
            "Batch:109\n",
            "d_loss:0.6060474216938019\n",
            "g_loss:[0.7500458, 0.7450927, 0.0024765395]\n",
            "Batch:110\n",
            "d_loss:0.6027539372444153\n",
            "g_loss:[0.7286181, 0.7245847, 0.0020166815]\n",
            "Batch:111\n",
            "d_loss:0.6828660070896149\n",
            "g_loss:[0.6986279, 0.69222647, 0.0032007105]\n",
            "Batch:112\n",
            "d_loss:0.623985543847084\n",
            "g_loss:[0.8021382, 0.7961315, 0.0030033512]\n",
            "Batch:113\n",
            "d_loss:0.6872945427894592\n",
            "g_loss:[0.8044779, 0.79806185, 0.0032080044]\n",
            "Batch:114\n",
            "d_loss:0.6176026314496994\n",
            "g_loss:[0.88973594, 0.88520026, 0.0022678322]\n",
            "Batch:115\n",
            "d_loss:0.6138221621513367\n",
            "g_loss:[0.7985087, 0.7933812, 0.002563734]\n",
            "Batch:116\n",
            "d_loss:0.5886039733886719\n",
            "g_loss:[0.8739861, 0.86899793, 0.0024941063]\n",
            "Batch:117\n",
            "d_loss:0.6648505628108978\n",
            "g_loss:[0.75741553, 0.75332385, 0.0020458456]\n",
            "Batch:118\n",
            "d_loss:0.6434342116117477\n",
            "g_loss:[0.79944074, 0.7942188, 0.0026109735]\n",
            "Batch:119\n",
            "d_loss:0.6205568611621857\n",
            "g_loss:[0.8029097, 0.7969587, 0.0029755014]\n",
            "Batch:120\n",
            "d_loss:0.6174487620592117\n",
            "g_loss:[0.8360489, 0.83214843, 0.0019502293]\n",
            "Batch:121\n",
            "d_loss:0.6837161034345627\n",
            "g_loss:[0.7631377, 0.75870025, 0.0022187114]\n",
            "Batch:122\n",
            "d_loss:0.6088145226240158\n",
            "g_loss:[0.7676597, 0.7643281, 0.0016658005]\n",
            "Batch:123\n",
            "d_loss:0.5985037833452225\n",
            "g_loss:[0.87244225, 0.8697543, 0.0013439571]\n",
            "Batch:124\n",
            "d_loss:0.5928211361169815\n",
            "g_loss:[0.7477741, 0.7445426, 0.0016157569]\n",
            "Batch:125\n",
            "d_loss:0.6290084570646286\n",
            "g_loss:[0.80982184, 0.80614555, 0.0018381528]\n",
            "Batch:126\n",
            "d_loss:0.6238934099674225\n",
            "g_loss:[0.8128842, 0.8086699, 0.0021071557]\n",
            "Batch:127\n",
            "d_loss:0.5915983766317368\n",
            "g_loss:[0.8378921, 0.83308834, 0.002401889]\n",
            "Batch:128\n",
            "d_loss:0.6058187931776047\n",
            "g_loss:[0.8101415, 0.80546355, 0.002338977]\n",
            "Batch:129\n",
            "d_loss:0.6850014477968216\n",
            "g_loss:[0.8440284, 0.8384332, 0.002797605]\n",
            "Batch:130\n",
            "d_loss:0.605068176984787\n",
            "g_loss:[0.7673493, 0.76233125, 0.0025090175]\n",
            "Batch:131\n",
            "d_loss:0.6185261607170105\n",
            "g_loss:[0.744668, 0.73990905, 0.0023794672]\n",
            "Batch:132\n",
            "d_loss:0.6005080789327621\n",
            "g_loss:[0.8222781, 0.81938636, 0.0014458465]\n",
            "Batch:133\n",
            "d_loss:0.6139502227306366\n",
            "g_loss:[0.7699666, 0.7665442, 0.0017111799]\n",
            "Batch:134\n",
            "d_loss:0.6318822801113129\n",
            "g_loss:[0.81456196, 0.8101947, 0.002183652]\n",
            "Batch:135\n",
            "d_loss:0.6203083544969559\n",
            "g_loss:[0.7633524, 0.7584221, 0.002465163]\n",
            "Batch:136\n",
            "d_loss:0.6329998672008514\n",
            "g_loss:[0.7117435, 0.7081179, 0.0018127752]\n",
            "Batch:137\n",
            "d_loss:0.5696994811296463\n",
            "g_loss:[0.70748025, 0.704364, 0.0015581129]\n",
            "Batch:138\n",
            "d_loss:0.6484663188457489\n",
            "g_loss:[0.7759898, 0.77338654, 0.0013016182]\n",
            "Batch:139\n",
            "d_loss:0.6273075193166733\n",
            "g_loss:[0.75880456, 0.75609136, 0.0013566059]\n",
            "Batch:140\n",
            "d_loss:0.6232578307390213\n",
            "g_loss:[0.7252208, 0.72079337, 0.0022137214]\n",
            "Batch:141\n",
            "d_loss:0.6445525139570236\n",
            "g_loss:[0.7623433, 0.75755924, 0.0023920385]\n",
            "Batch:142\n",
            "d_loss:0.5853355377912521\n",
            "g_loss:[1.9711721, 1.9676301, 0.0017709684]\n",
            "Batch:143\n",
            "d_loss:1.1550074070692062\n",
            "g_loss:[1.0044242, 1.0008591, 0.0017825578]\n",
            "Batch:144\n",
            "d_loss:0.7330000400543213\n",
            "g_loss:[0.9353284, 0.9322147, 0.0015568712]\n",
            "Batch:145\n",
            "d_loss:0.6535174250602722\n",
            "g_loss:[0.98711056, 0.9837264, 0.0016920955]\n",
            "Batch:146\n",
            "d_loss:0.6700202226638794\n",
            "g_loss:[1.2149845, 1.2107378, 0.0021233486]\n",
            "Batch:147\n",
            "d_loss:0.649393692612648\n",
            "g_loss:[1.125994, 1.1225548, 0.001719609]\n",
            "Batch:148\n",
            "d_loss:0.6643655300140381\n",
            "g_loss:[1.034235, 1.0308143, 0.0017103733]\n",
            "Batch:149\n",
            "d_loss:0.7898635715246201\n",
            "g_loss:[1.0139314, 1.010009, 0.0019611884]\n",
            "Batch:150\n",
            "d_loss:0.6706142276525497\n",
            "g_loss:[0.9181531, 0.9140465, 0.0020532878]\n",
            "Batch:151\n",
            "d_loss:0.6947853267192841\n",
            "g_loss:[0.98026145, 0.9766897, 0.0017858625]\n",
            "Batch:152\n",
            "d_loss:0.7909729480743408\n",
            "g_loss:[1.0341781, 1.0307305, 0.0017238526]\n",
            "Batch:153\n",
            "d_loss:0.8102512061595917\n",
            "g_loss:[1.020448, 1.0174861, 0.0014809429]\n",
            "Batch:154\n",
            "d_loss:0.672765463590622\n",
            "g_loss:[0.92988527, 0.9267703, 0.0015574611]\n",
            "Batch:155\n",
            "d_loss:0.6310786604881287\n",
            "g_loss:[1.0016475, 0.9981735, 0.0017369773]\n",
            "Batch:156\n",
            "d_loss:0.679203525185585\n",
            "g_loss:[0.93681633, 0.93308425, 0.0018660526]\n",
            "Batch:157\n",
            "d_loss:0.6931680589914322\n",
            "g_loss:[0.76824313, 0.7651744, 0.001534384]\n",
            "Batch:158\n",
            "d_loss:0.6104154288768768\n",
            "g_loss:[0.9897037, 0.9864714, 0.0016161475]\n",
            "Batch:159\n",
            "d_loss:0.6121761053800583\n",
            "g_loss:[0.8855431, 0.8820361, 0.0017535206]\n",
            "Batch:160\n",
            "d_loss:0.6350677460432053\n",
            "g_loss:[0.85514146, 0.85154206, 0.0017997152]\n",
            "Batch:161\n",
            "d_loss:0.6234868764877319\n",
            "g_loss:[0.92629206, 0.92266405, 0.0018140178]\n",
            "Batch:162\n",
            "d_loss:0.708092600107193\n",
            "g_loss:[0.96715933, 0.96366453, 0.0017473873]\n",
            "Batch:163\n",
            "d_loss:0.7072229385375977\n",
            "g_loss:[0.86291194, 0.8602672, 0.0013223451]\n",
            "Batch:164\n",
            "d_loss:0.662730410695076\n",
            "g_loss:[0.8274481, 0.8244995, 0.001474319]\n",
            "Batch:165\n",
            "d_loss:0.6109090447425842\n",
            "g_loss:[0.7791287, 0.7757484, 0.0016901421]\n",
            "Batch:166\n",
            "d_loss:0.5860883742570877\n",
            "g_loss:[0.7464141, 0.74273795, 0.0018380808]\n",
            "Batch:167\n",
            "d_loss:0.5909463763237\n",
            "g_loss:[0.79108435, 0.78748524, 0.0017995519]\n",
            "Batch:168\n",
            "d_loss:0.6022259294986725\n",
            "g_loss:[0.78580546, 0.7822914, 0.0017570404]\n",
            "Batch:169\n",
            "d_loss:0.6116575449705124\n",
            "g_loss:[0.76024455, 0.75711447, 0.0015650372]\n",
            "Batch:170\n",
            "d_loss:0.6002974212169647\n",
            "g_loss:[0.7455048, 0.74240226, 0.0015512621]\n",
            "Batch:171\n",
            "d_loss:0.6138308644294739\n",
            "g_loss:[0.7376005, 0.73532426, 0.0011381172]\n",
            "Batch:172\n",
            "d_loss:0.5933028757572174\n",
            "g_loss:[0.7406742, 0.73771775, 0.0014782154]\n",
            "Batch:173\n",
            "d_loss:0.6017636656761169\n",
            "g_loss:[0.7344502, 0.7307855, 0.0018323755]\n",
            "Batch:174\n",
            "d_loss:0.5993059128522873\n",
            "g_loss:[0.797707, 0.79396707, 0.0018699712]\n",
            "Batch:175\n",
            "d_loss:0.687835767865181\n",
            "g_loss:[0.7549653, 0.7517314, 0.0016169614]\n",
            "Batch:176\n",
            "d_loss:0.6090450584888458\n",
            "g_loss:[0.73101324, 0.72791046, 0.0015513878]\n",
            "Batch:177\n",
            "d_loss:0.5801502168178558\n",
            "g_loss:[0.81837857, 0.8141741, 0.0021022176]\n",
            "Batch:178\n",
            "d_loss:0.584062859416008\n",
            "g_loss:[0.81848675, 0.814101, 0.002192878]\n",
            "Batch:179\n",
            "d_loss:0.6307844072580338\n",
            "g_loss:[0.8332974, 0.829587, 0.0018551904]\n",
            "Batch:180\n",
            "d_loss:0.607778400182724\n",
            "g_loss:[0.8796344, 0.87615883, 0.0017377757]\n",
            "Batch:181\n",
            "d_loss:0.6194548308849335\n",
            "g_loss:[0.90973246, 0.90686965, 0.0014314093]\n",
            "Batch:182\n",
            "d_loss:0.6344114392995834\n",
            "g_loss:[0.7839669, 0.7809771, 0.0014949099]\n",
            "Batch:183\n",
            "d_loss:0.5989528298377991\n",
            "g_loss:[0.78060204, 0.77568424, 0.0024589011]\n",
            "Batch:184\n",
            "d_loss:0.6305868625640869\n",
            "g_loss:[0.80512625, 0.80086863, 0.0021288036]\n",
            "Batch:185\n",
            "d_loss:0.5793583542108536\n",
            "g_loss:[0.7776362, 0.7732441, 0.0021960468]\n",
            "Batch:186\n",
            "d_loss:0.6312394589185715\n",
            "g_loss:[0.7937504, 0.7888477, 0.0024513672]\n",
            "Batch:187\n",
            "d_loss:0.582784965634346\n",
            "g_loss:[0.7392121, 0.73236024, 0.0034259167]\n",
            "Batch:188\n",
            "d_loss:0.6277629882097244\n",
            "g_loss:[0.8035473, 0.7998849, 0.0018311932]\n",
            "Batch:189\n",
            "d_loss:0.7521206438541412\n",
            "g_loss:[0.8164968, 0.81263435, 0.001931228]\n",
            "Batch:190\n",
            "d_loss:0.6961227655410767\n",
            "g_loss:[1.2796743, 1.2752194, 0.0022274489]\n",
            "Batch:191\n",
            "d_loss:0.6689340174198151\n",
            "g_loss:[1.1217171, 1.1165671, 0.0025749945]\n",
            "Batch:192\n",
            "d_loss:0.6745012700557709\n",
            "g_loss:[0.94562066, 0.94152236, 0.002049141]\n",
            "Batch:193\n",
            "d_loss:0.6463631093502045\n",
            "g_loss:[1.054727, 1.0509248, 0.0019011069]\n",
            "Batch:194\n",
            "d_loss:0.6974378228187561\n",
            "g_loss:[0.88868165, 0.88535666, 0.0016624815]\n",
            "Batch:195\n",
            "d_loss:0.6314989477396011\n",
            "g_loss:[0.94545186, 0.94144094, 0.0020054684]\n",
            "Batch:196\n",
            "d_loss:0.6798887252807617\n",
            "g_loss:[0.7973379, 0.79380715, 0.0017653665]\n",
            "Batch:197\n",
            "d_loss:0.5716142058372498\n",
            "g_loss:[0.8507421, 0.8470124, 0.0018648368]\n",
            "Batch:198\n",
            "d_loss:0.6368791610002518\n",
            "g_loss:[0.7438714, 0.73963815, 0.0021166308]\n",
            "Batch:199\n",
            "d_loss:0.6130359023809433\n",
            "g_loss:[0.7757808, 0.771809, 0.001985908]\n",
            "Batch:200\n",
            "d_loss:0.613349199295044\n",
            "g_loss:[0.74210876, 0.73896104, 0.001573853]\n",
            "Batch:201\n",
            "d_loss:0.6286117881536484\n",
            "g_loss:[0.78427094, 0.7815198, 0.0013755911]\n",
            "Batch:202\n",
            "d_loss:0.653906375169754\n",
            "g_loss:[0.76632714, 0.76316166, 0.0015827562]\n",
            "Batch:203\n",
            "d_loss:0.602954313158989\n",
            "g_loss:[0.86310256, 0.8599467, 0.0015779043]\n",
            "Batch:204\n",
            "d_loss:0.6234243661165237\n",
            "g_loss:[0.74853194, 0.74563056, 0.0014507001]\n",
            "Batch:205\n",
            "d_loss:0.6238307058811188\n",
            "g_loss:[0.8134412, 0.8101311, 0.0016550613]\n",
            "Batch:206\n",
            "d_loss:0.5908036828041077\n",
            "g_loss:[0.8045485, 0.8016957, 0.0014264015]\n",
            "Batch:207\n",
            "d_loss:0.6163288950920105\n",
            "g_loss:[1.0634948, 1.0604134, 0.0015407254]\n",
            "Batch:208\n",
            "d_loss:0.6552974432706833\n",
            "g_loss:[0.7363223, 0.7331062, 0.0016080497]\n",
            "Batch:209\n",
            "d_loss:0.6044420003890991\n",
            "g_loss:[0.9741442, 0.9718195, 0.0011623405]\n",
            "Batch:210\n",
            "d_loss:0.6562866717576981\n",
            "g_loss:[0.7368225, 0.73434335, 0.0012395602]\n",
            "Batch:211\n",
            "d_loss:0.6026389300823212\n",
            "g_loss:[0.75157565, 0.74922264, 0.0011764967]\n",
            "Batch:212\n",
            "d_loss:0.5721737593412399\n",
            "g_loss:[0.84404284, 0.8407899, 0.0016264633]\n",
            "Batch:213\n",
            "d_loss:0.6307746917009354\n",
            "g_loss:[0.92635524, 0.92273283, 0.0018112105]\n",
            "Batch:214\n",
            "d_loss:0.5864914953708649\n",
            "g_loss:[0.701309, 0.69819385, 0.0015576023]\n",
            "Batch:215\n",
            "d_loss:0.5804267674684525\n",
            "g_loss:[0.7545635, 0.7515783, 0.0014926211]\n",
            "Batch:216\n",
            "d_loss:0.6537447720766068\n",
            "g_loss:[0.7258796, 0.7229034, 0.0014881269]\n",
            "Batch:217\n",
            "d_loss:0.5697251856327057\n",
            "g_loss:[0.77334464, 0.77068365, 0.0013304915]\n",
            "Batch:218\n",
            "d_loss:0.6851765215396881\n",
            "g_loss:[0.76710284, 0.7640883, 0.0015072703]\n",
            "Batch:219\n",
            "d_loss:0.5991235673427582\n",
            "g_loss:[0.70138663, 0.69863683, 0.0013748973]\n",
            "Batch:220\n",
            "d_loss:0.5948799550533295\n",
            "g_loss:[0.76489204, 0.76128674, 0.0018026391]\n",
            "Batch:221\n",
            "d_loss:0.5557016730308533\n",
            "g_loss:[0.7661194, 0.7627461, 0.0016866721]\n",
            "Batch:222\n",
            "d_loss:0.6245597302913666\n",
            "g_loss:[0.727958, 0.7248173, 0.0015703801]\n",
            "Batch:223\n",
            "d_loss:0.6237691044807434\n",
            "g_loss:[0.8245511, 0.82184726, 0.0013519186]\n",
            "Batch:224\n",
            "d_loss:0.7794934660196304\n",
            "g_loss:[0.92265224, 0.9191826, 0.0017348229]\n",
            "Batch:225\n",
            "d_loss:0.6178998798131943\n",
            "g_loss:[0.89273536, 0.88984954, 0.0014428981]\n",
            "Batch:226\n",
            "d_loss:0.660406693816185\n",
            "g_loss:[0.85645115, 0.8532225, 0.0016143362]\n",
            "Batch:227\n",
            "d_loss:0.5670060813426971\n",
            "g_loss:[0.78730625, 0.7831245, 0.0020908772]\n",
            "Batch:228\n",
            "d_loss:0.6558580994606018\n",
            "g_loss:[0.8007788, 0.79763854, 0.001570148]\n",
            "Batch:229\n",
            "d_loss:0.603762149810791\n",
            "g_loss:[0.7649459, 0.76160824, 0.0016688411]\n",
            "Batch:230\n",
            "d_loss:0.5856941640377045\n",
            "g_loss:[0.7571538, 0.75383264, 0.001660597]\n",
            "Batch:231\n",
            "d_loss:0.6361244022846222\n",
            "g_loss:[0.7309294, 0.7274504, 0.0017395158]\n",
            "Batch:232\n",
            "d_loss:0.6031004786491394\n",
            "g_loss:[0.745589, 0.7422937, 0.0016476559]\n",
            "Batch:233\n",
            "d_loss:0.5990646779537201\n",
            "g_loss:[0.7230383, 0.7202647, 0.0013868324]\n",
            "Batch:234\n",
            "d_loss:0.5874953716993332\n",
            "g_loss:[0.7436787, 0.74052703, 0.0015758249]\n",
            "Batch:235\n",
            "d_loss:0.5613070726394653\n",
            "g_loss:[0.7439624, 0.74115384, 0.0014042836]\n",
            "Batch:236\n",
            "d_loss:0.5891416221857071\n",
            "g_loss:[0.71788067, 0.71503055, 0.001425048]\n",
            "Batch:237\n",
            "d_loss:0.5978550612926483\n",
            "g_loss:[0.7031429, 0.7003776, 0.0013826638]\n",
            "Batch:238\n",
            "d_loss:0.5527135282754898\n",
            "g_loss:[0.73860204, 0.7365947, 0.0010036697]\n",
            "Batch:239\n",
            "d_loss:0.568995401263237\n",
            "g_loss:[0.73213875, 0.729196, 0.0014713776]\n",
            "Batch:240\n",
            "d_loss:0.6257032305002213\n",
            "g_loss:[0.71628064, 0.7136889, 0.0012958723]\n",
            "Batch:241\n",
            "d_loss:0.5946419537067413\n",
            "g_loss:[0.7601127, 0.7574421, 0.0013352978]\n",
            "Batch:242\n",
            "d_loss:0.5830337703227997\n",
            "g_loss:[0.77090234, 0.7676189, 0.0016417259]\n",
            "Batch:243\n",
            "d_loss:0.5903947800397873\n",
            "g_loss:[0.7646531, 0.76161987, 0.0015165983]\n",
            "Batch:244\n",
            "d_loss:0.5820368826389313\n",
            "g_loss:[0.74103475, 0.7385084, 0.0012631585]\n",
            "Batch:245\n",
            "d_loss:0.6393389254808426\n",
            "g_loss:[0.7632001, 0.76060104, 0.0012995398]\n",
            "Batch:246\n",
            "d_loss:0.6229441463947296\n",
            "g_loss:[0.70467365, 0.70275414, 0.0009597395]\n",
            "Batch:247\n",
            "d_loss:0.6065890341997147\n",
            "g_loss:[0.7024981, 0.7002127, 0.0011426788]\n",
            "Batch:248\n",
            "d_loss:0.5647299736738205\n",
            "g_loss:[0.80181843, 0.79906523, 0.0013766016]\n",
            "Batch:249\n",
            "d_loss:0.5590822100639343\n",
            "g_loss:[0.70908695, 0.70618516, 0.0014509079]\n",
            "Batch:250\n",
            "d_loss:0.5943351089954376\n",
            "g_loss:[0.7357778, 0.73321044, 0.0012836845]\n",
            "Batch:251\n",
            "d_loss:0.5831151604652405\n",
            "g_loss:[0.74840146, 0.7457912, 0.0013051196]\n",
            "Batch:252\n",
            "d_loss:0.6075528860092163\n",
            "g_loss:[0.80708766, 0.8045839, 0.0012518661]\n",
            "Batch:253\n",
            "d_loss:0.5859435796737671\n",
            "g_loss:[0.7864099, 0.783714, 0.0013479528]\n",
            "Batch:254\n",
            "d_loss:0.6576458066701889\n",
            "g_loss:[0.76166064, 0.7590302, 0.0013152175]\n",
            "Batch:255\n",
            "d_loss:0.5962823331356049\n",
            "g_loss:[0.77242965, 0.7686925, 0.001868574]\n",
            "Batch:256\n",
            "d_loss:0.5831510573625565\n",
            "g_loss:[0.7856934, 0.78102636, 0.0023335258]\n",
            "Batch:257\n",
            "d_loss:0.5964237451553345\n",
            "g_loss:[0.7460218, 0.74178636, 0.0021177302]\n",
            "Batch:258\n",
            "d_loss:0.5688472837209702\n",
            "g_loss:[0.79114646, 0.78776973, 0.0016883655]\n",
            "Batch:259\n",
            "d_loss:0.6838231086730957\n",
            "g_loss:[0.7056335, 0.70292604, 0.0013537306]\n",
            "Batch:260\n",
            "d_loss:0.626069501042366\n",
            "g_loss:[0.7228102, 0.72034705, 0.0012315887]\n",
            "Batch:261\n",
            "d_loss:0.5948570817708969\n",
            "g_loss:[0.734869, 0.7313925, 0.0017382452]\n",
            "Batch:262\n",
            "d_loss:0.5766925811767578\n",
            "g_loss:[0.72827667, 0.7250359, 0.0016203693]\n",
            "Batch:263\n",
            "d_loss:0.5933508574962616\n",
            "g_loss:[0.7986823, 0.7954383, 0.0016220037]\n",
            "Batch:264\n",
            "d_loss:0.6083318591117859\n",
            "g_loss:[0.713548, 0.71085113, 0.0013484313]\n",
            "Batch:265\n",
            "d_loss:0.5680313855409622\n",
            "g_loss:[0.7370687, 0.73515093, 0.000958878]\n",
            "Batch:266\n",
            "d_loss:0.5945645421743393\n",
            "g_loss:[0.7333545, 0.73077, 0.0012922599]\n",
            "Batch:267\n",
            "d_loss:0.5577390417456627\n",
            "g_loss:[1.349589, 1.347131, 0.0012289605]\n",
            "Batch:268\n",
            "d_loss:1.4067712426185608\n",
            "g_loss:[0.8661388, 0.8637956, 0.0011716204]\n",
            "Batch:269\n",
            "d_loss:0.6769402325153351\n",
            "g_loss:[0.97272646, 0.9706908, 0.0010178487]\n",
            "Batch:270\n",
            "d_loss:0.7004760801792145\n",
            "g_loss:[0.90895236, 0.9069544, 0.000998959]\n",
            "Batch:271\n",
            "d_loss:0.6682665944099426\n",
            "g_loss:[0.8242727, 0.8221412, 0.0010657593]\n",
            "Batch:272\n",
            "d_loss:0.6455044150352478\n",
            "g_loss:[0.8335591, 0.8318382, 0.0008604479]\n",
            "Batch:273\n",
            "d_loss:0.656910315155983\n",
            "g_loss:[0.9544847, 0.9525666, 0.0009590327]\n",
            "Batch:274\n",
            "d_loss:0.7266681492328644\n",
            "g_loss:[0.856179, 0.8539073, 0.0011358656]\n",
            "Batch:275\n",
            "d_loss:0.6558763384819031\n",
            "g_loss:[0.7856327, 0.7837507, 0.00094101223]\n",
            "Batch:276\n",
            "d_loss:0.6867608428001404\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "g_loss:[0.8503362, 0.8476915, 0.0013223679]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}